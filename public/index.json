[{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/federal-reserve-data/","section":"Topics","summary":"","title":"Federal Reserve Data"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/","section":"Jared Szajkowski","summary":"","title":"Jared Szajkowski"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/pandas/","section":"Topics","summary":"","title":"Pandas"},{"content":"Introduction #In this post, we will look into the Fed Funds cycles and evaluate asset class performance during loosening and tightening of monetary policy.\nPython Functions #Here are the functions needed for this project:\ncalc_fed_cycle_asset_performance: Calculates the performance of various asset classes during the Fed Funds cycles. df_info: A simple function to display the information about a DataFrame and the first five rows and last five rows. df_info_markdown: Similar to the df_info function above, except that it coverts the output to markdown. export_track_md_deps: Exports various text outputs to markdown files, which are included in the index.md file created when building the site with Hugo. load_data: Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. pandas_set_decimal_places: Set the number of decimal places displayed for floating-point numbers in pandas. plot_bar_returns_ffr_change: Plot the bar chart of the cumulative or annualized returns for the asset class along with the change in the Fed Funds Rate. plot_timeseries: Plot the timeseries data from a DataFrame for a specified date range and columns. plot_scatter_regression_ffr_vs_returns: Plot the scatter plot and regression of the annualized return for the asset class along with the annualized change in the Fed Funds Rate. yf_pull_data: Download daily price data from Yahoo Finance and export it. Data Overview #Acquire \u0026amp; Plot Fed Funds Data #First, let\u0026rsquo;s get the data for the Fed Funds rate (FFR):\n# Set decimal places pandas_set_decimal_places(4) # Pull Effective Fed Funds Rate from FRED fedfunds = web.DataReader(\u0026#34;FEDFUNDS\u0026#34;, \u0026#34;fred\u0026#34;, start=\u0026#34;1900-01-01\u0026#34;, end=datetime.today()) fedfunds[\u0026#34;FEDFUNDS\u0026#34;] = fedfunds[\u0026#34;FEDFUNDS\u0026#34;] / 100 # Convert to decimal # Resample to monthly frequency and compute change in rate fedfunds_monthly = fedfunds.resample(\u0026#34;M\u0026#34;).last() fedfunds_monthly = fedfunds_monthly[(fedfunds_monthly.index \u0026gt;= pd.to_datetime(start_date)) \u0026amp; (fedfunds_monthly.index \u0026lt;= pd.to_datetime(end_date))] fedfunds_monthly[\u0026#34;FedFunds_Change\u0026#34;] = fedfunds_monthly[\u0026#34;FEDFUNDS\u0026#34;].diff() This gives us:\nThe columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 252 entries, 2004-11-30 to 2025-10-31 Freq: ME Data columns (total 2 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 FEDFUNDS 252 non-null float64 1 FedFunds_Change 251 non-null float64 dtypes: float64(2) memory usage: 5.9 KB The first 5 rows are: | DATE | FEDFUNDS | FedFunds_Change | |:--------------------|-----------:|------------------:| | 2004-11-30 00:00:00 | 0.0193 | nan | | 2004-12-31 00:00:00 | 0.0216 | 0.0023 | | 2005-01-31 00:00:00 | 0.0228 | 0.0012 | | 2005-02-28 00:00:00 | 0.0250 | 0.0022 | | 2005-03-31 00:00:00 | 0.0263 | 0.0013 | The last 5 rows are: | DATE | FEDFUNDS | FedFunds_Change | |:--------------------|-----------:|------------------:| | 2025-06-30 00:00:00 | 0.0433 | 0.0000 | | 2025-07-31 00:00:00 | 0.0433 | 0.0000 | | 2025-08-31 00:00:00 | 0.0433 | 0.0000 | | 2025-09-30 00:00:00 | 0.0422 | -0.0011 | | 2025-10-31 00:00:00 | 0.0409 | -0.0013 | We can then generate several useful visual aids (plots). First, the FFR from the beginning of our data set (11/2004):\nplot_timeseries( price_df=fedfunds_monthly, plot_start_date=start_date, plot_end_date=end_date, plot_columns=[\u0026#34;FEDFUNDS\u0026#34;], title=\u0026#34;Fed Funds Rate\u0026#34;, x_label=\u0026#34;Date\u0026#34;, x_format=\u0026#34;Year\u0026#34;, y_label=\u0026#34;Rate (%)\u0026#34;, y_format=\u0026#34;Percentage\u0026#34;, y_format_decimal_places=1, y_tick_spacing=0.005, grid=True, legend=False, export_plot=True, plot_file_name=\u0026#34;01_Fed_Funds_Rate\u0026#34;, ) And then the change in FFR from month-to-month:\nplot_timeseries( price_df=fedfunds_monthly, plot_start_date=start_date, plot_end_date=end_date, plot_columns=[\u0026#34;FedFunds_Change\u0026#34;], title=\u0026#34;Fed Funds Change In Rate\u0026#34;, x_label=\u0026#34;Date\u0026#34;, x_format=\u0026#34;Year\u0026#34;, y_label=\u0026#34;Rate (%)\u0026#34;, y_format=\u0026#34;Percentage\u0026#34;, y_format_decimal_places=2, y_tick_spacing=0.0025, grid=True, legend=False, export_plot=True, plot_file_name=\u0026#34;01_Fed_Funds_Change_In_Rate\u0026#34;, ) This plot, in particular, makes it easy to show the monthly increase and decrease in the FFR, as well as the magnitude of the change (i.e. slow, drawn-out increases or decreases or abrupt large increases or decreases).\nDefine Fed Policy Cycles #Next, we will define the Fed policy tightening and loosening cycles. This is done via visual inspection of the FFR plot and establishing some timeframes for when the cycles started and ended. Here\u0026rsquo;s the list of cycles:\n# Define manually specified Fed policy cycles fed_cycles = [ (\u0026#34;2004-11-01\u0026#34;, \u0026#34;2006-07-01\u0026#34;), (\u0026#34;2006-07-01\u0026#34;, \u0026#34;2007-07-01\u0026#34;), (\u0026#34;2007-07-01\u0026#34;, \u0026#34;2008-12-01\u0026#34;), (\u0026#34;2008-12-01\u0026#34;, \u0026#34;2015-11-01\u0026#34;), (\u0026#34;2015-11-01\u0026#34;, \u0026#34;2019-01-01\u0026#34;), (\u0026#34;2019-01-01\u0026#34;, \u0026#34;2019-07-01\u0026#34;), (\u0026#34;2019-07-01\u0026#34;, \u0026#34;2020-04-01\u0026#34;), (\u0026#34;2020-04-01\u0026#34;, \u0026#34;2022-02-01\u0026#34;), (\u0026#34;2022-02-01\u0026#34;, \u0026#34;2023-08-01\u0026#34;), (\u0026#34;2023-08-01\u0026#34;, \u0026#34;2024-08-01\u0026#34;), (\u0026#34;2024-08-01\u0026#34;, datetime.today().strftime(\u0026#39;%Y-%m-%d\u0026#39;)), ] # Optional: assign a name to each cycle cycle_labels = [f\u0026#34;Cycle {i+1}\u0026#34; for i in range(len(fed_cycles))] And here\u0026rsquo;s the list of total change in the FFR corresponding to each cycle:\n# Set decimal places pandas_set_decimal_places(4) # Calc changes by fed cycle defined above fed_changes = [] for (start, end) in fed_cycles: start = pd.to_datetime(start) end = pd.to_datetime(end) try: rate_start = fedfunds.loc[start, \u0026#34;FEDFUNDS\u0026#34;] except KeyError: rate_start = fedfunds.loc[:start].iloc[-1][\u0026#34;FEDFUNDS\u0026#34;] try: rate_end = fedfunds.loc[end, \u0026#34;FEDFUNDS\u0026#34;] except KeyError: rate_end = fedfunds.loc[:end].iloc[-1][\u0026#34;FEDFUNDS\u0026#34;] change = rate_end - rate_start fed_changes.append(change) fed_changes_df = pd.DataFrame({ \u0026#34;Cycle\u0026#34;: cycle_labels, \u0026#34;FedFunds_Change\u0026#34;: fed_changes }) Which gives us the following cycles and cumulative change in rate per cycle:\n| | Cycle | FedFunds_Change | |---:|:---------|------------------:| | 0 | Cycle 1 | 0.0331 | | 1 | Cycle 2 | 0.0002 | | 2 | Cycle 3 | -0.0510 | | 3 | Cycle 4 | -0.0004 | | 4 | Cycle 5 | 0.0228 | | 5 | Cycle 6 | 0.0000 | | 6 | Cycle 7 | -0.0235 | | 7 | Cycle 8 | 0.0003 | | 8 | Cycle 9 | 0.0525 | | 9 | Cycle 10 | 0.0000 | | 10 | Cycle 11 | -0.0145 | Return Performance By Fed Policy Cycle #Moving on, we will now look at the performance of three (3) different asset classes during each Fed cycle. We\u0026rsquo;ll use SPY as a proxy for stocks, TLT as a proxy for bonds, and GLD as a proxy for gold. These datasets are slightly limiting due to the availability of all 3 starting in late 2004, but will work for our simple exercise. In a future post, we\u0026rsquo;ll look to use Bloomberg indices instead.\nStocks (SPY) #First, we pull data for SPY with the following:\n# Set decimal places pandas_set_decimal_places(2) yf_pull_data( base_directory=DATA_DIR, ticker=\u0026#34;SPY\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Exchange_Traded_Funds\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) And then load data with the following:\nspy = load_data( base_directory=DATA_DIR, ticker=\u0026#34;SPY\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Exchange_Traded_Funds\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, file_format=\u0026#34;pickle\u0026#34;, ) # Filter SPY to date range spy = spy[(spy.index \u0026gt;= pd.to_datetime(start_date)) \u0026amp; (spy.index \u0026lt;= pd.to_datetime(end_date))] # Resample to monthly frequency spy_monthly = spy.resample(\u0026#34;M\u0026#34;).last() spy_monthly[\u0026#34;Monthly_Return\u0026#34;] = spy_monthly[\u0026#34;Close\u0026#34;].pct_change() Which gives us the following:\nThe columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 252 entries, 2004-11-30 to 2025-10-31 Freq: ME Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Close 252 non-null float64 1 High 252 non-null float64 2 Low 252 non-null float64 3 Open 252 non-null float64 4 Volume 252 non-null int64 5 Monthly_Return 251 non-null float64 dtypes: float64(5), int64(1) memory usage: 13.8 KB The first 5 rows are: | Date | Close | High | Low | Open | Volume | Monthly_Return | |:--------------------|--------:|-------:|------:|-------:|------------:|-----------------:| | 2004-11-30 00:00:00 | 79.60 | 79.83 | 79.43 | 79.67 | 53685200.00 | nan | | 2004-12-31 00:00:00 | 81.99 | 82.53 | 81.95 | 82.28 | 28648800.00 | 0.03 | | 2005-01-31 00:00:00 | 80.15 | 80.22 | 79.85 | 80.01 | 52532700.00 | -0.02 | | 2005-02-28 00:00:00 | 81.83 | 82.28 | 81.43 | 82.18 | 69381300.00 | 0.02 | | 2005-03-31 00:00:00 | 80.33 | 80.67 | 80.27 | 80.49 | 64575400.00 | -0.02 | The last 5 rows are: | Date | Close | High | Low | Open | Volume | Monthly_Return | |:--------------------|--------:|-------:|-------:|-------:|-------------:|-----------------:| | 2025-06-30 00:00:00 | 614.33 | 615.69 | 611.53 | 613.86 | 92502500.00 | 0.05 | | 2025-07-31 00:00:00 | 628.48 | 636.20 | 627.17 | 635.81 | 103385200.00 | 0.02 | | 2025-08-31 00:00:00 | 641.37 | 644.15 | 639.47 | 643.78 | 74522200.00 | 0.02 | | 2025-09-30 00:00:00 | 664.22 | 664.69 | 659.66 | 660.98 | 86288000.00 | 0.04 | | 2025-10-31 00:00:00 | 680.05 | 683.06 | 677.24 | 683.02 | 87164100.00 | 0.02 | Next, we can plot the price history before calculating the cycle performance:\nplot_timeseries( price_df=spy, plot_start_date=start_date, plot_end_date=end_date, plot_columns=[\u0026#34;Close\u0026#34;], title=\u0026#34;SPY Close Price\u0026#34;, x_label=\u0026#34;Date\u0026#34;, x_format=\u0026#34;Year\u0026#34;, y_label=\u0026#34;Price ($)\u0026#34;, y_format=\u0026#34;Decimal\u0026#34;, y_tick_spacing=50, grid=True, legend=False, export_plot=True, plot_file_name=\u0026#34;02_SPY_Price\u0026#34;, y_format_decimal_places=0, ) Next, we will calculate the performance for SPY based on the pre-defined Fed cycles:\nspy_cycle_df = calc_fed_cycle_asset_performance( fed_cycles=fed_cycles, cycle_labels=cycle_labels, fed_changes=fed_changes, monthly_df=spy_monthly, ) Which gives us:\n| | Cycle | Start | End | Months | CumulativeReturn | CumulativeReturnPct | AverageMonthlyReturn | AverageMonthlyReturnPct | AnnualizedReturn | AnnualizedReturnPct | Volatility | FedFundsChange | FedFundsChange_bps | FFR_AnnualizedChange | FFR_AnnualizedChange_bps | Label | |---:|:---------|:-----------|:-----------|---------:|-------------------:|----------------------:|-----------------------:|--------------------------:|-------------------:|----------------------:|-------------:|-----------------:|---------------------:|-----------------------:|---------------------------:|:-----------------------------------| | 0 | Cycle 1 | 2004-11-01 | 2006-07-01 | 20 | 0.11 | 11.32 | 0.01 | 0.59 | 0.07 | 6.64 | 0.08 | 0.03 | 331.00 | 0.02 | 198.60 | Cycle 1, 2004-11-01 to 2006-07-01 | | 1 | Cycle 2 | 2006-07-01 | 2007-07-01 | 12 | 0.20 | 20.36 | 0.02 | 1.57 | 0.20 | 20.36 | 0.07 | 0.00 | 2.00 | 0.00 | 2.00 | Cycle 2, 2006-07-01 to 2007-07-01 | | 2 | Cycle 3 | 2007-07-01 | 2008-12-01 | 17 | -0.39 | -38.55 | -0.03 | -2.67 | -0.29 | -29.09 | 0.19 | -0.05 | -510.00 | -0.04 | -360.00 | Cycle 3, 2007-07-01 to 2008-12-01 | | 3 | Cycle 4 | 2008-12-01 | 2015-11-01 | 83 | 1.67 | 167.34 | 0.01 | 1.28 | 0.15 | 15.28 | 0.15 | -0.00 | -4.00 | -0.00 | -0.58 | Cycle 4, 2008-12-01 to 2015-11-01 | | 4 | Cycle 5 | 2015-11-01 | 2019-01-01 | 38 | 0.28 | 28.30 | 0.01 | 0.70 | 0.08 | 8.19 | 0.11 | 0.02 | 228.00 | 0.01 | 72.00 | Cycle 5, 2015-11-01 to 2019-01-01 | | 5 | Cycle 6 | 2019-01-01 | 2019-07-01 | 6 | 0.18 | 18.33 | 0.03 | 2.95 | 0.40 | 40.01 | 0.18 | 0.00 | 0.00 | 0.00 | 0.00 | Cycle 6, 2019-01-01 to 2019-07-01 | | 6 | Cycle 7 | 2019-07-01 | 2020-04-01 | 9 | -0.11 | -10.67 | -0.01 | -1.10 | -0.14 | -13.96 | 0.19 | -0.02 | -235.00 | -0.03 | -313.33 | Cycle 7, 2019-07-01 to 2020-04-01 | | 7 | Cycle 8 | 2020-04-01 | 2022-02-01 | 22 | 0.79 | 79.13 | 0.03 | 2.78 | 0.37 | 37.43 | 0.16 | 0.00 | 3.00 | 0.00 | 1.64 | Cycle 8, 2020-04-01 to 2022-02-01 | | 8 | Cycle 9 | 2022-02-01 | 2023-08-01 | 18 | 0.04 | 4.18 | 0.00 | 0.40 | 0.03 | 2.77 | 0.21 | 0.05 | 525.00 | 0.03 | 350.00 | Cycle 9, 2022-02-01 to 2023-08-01 | | 9 | Cycle 10 | 2023-08-01 | 2024-08-01 | 12 | 0.22 | 22.00 | 0.02 | 1.75 | 0.22 | 22.00 | 0.15 | 0.00 | 0.00 | 0.00 | 0.00 | Cycle 10, 2023-08-01 to 2024-08-01 | | 10 | Cycle 11 | 2024-08-01 | 2025-12-26 | 15 | 0.26 | 25.72 | 0.02 | 1.59 | 0.20 | 20.09 | 0.11 | -0.01 | -145.00 | -0.01 | -116.00 | Cycle 11, 2024-08-01 to 2025-12-26 | This gives us the following data points:\nCycle start date Cycle end date Number of months in the cycle Cumulative return during the cycle (decimal and percent) Average monthly return during the cycle (decimal and percent) Annualized return during the cycle (decimal and percent) Return volatility during the cycle Cumulative change in FFR during the cycle (decimal and basis points) Annualized change in FFR during the cycle (decimal and basis points) From the above DataFrame, we can then plot the cumulative and annualized returns for each cycle in a bar chart. First, the cumulative returns along with the cumulative change in FFR:\nAnd then the annualized returns along with the annualized change in FFR:\nThe cumulative returns plot is not particularly insightful, but there are some interesting observations to be gained from the annualized returns plot. During the past two (2) rate cutting cycles (cycles 3 and 7), stocks have exhibited negative returns during the rate cutting cycle. However, after the rate cutting cycle was complete, returns during the following cycle (when rates were usually flat) were quite strong and higher than the historical mean return for the S\u0026amp;P 500. The economic intuition for this behavior is valid; as the economy weakens, investors are concerned about the pricing of equities, the returns become negative, and the Fed responds with cutting rates. The exact timing of when the Fed begins cutting rates is one of the unknowns; the Fed could be ahead of the curve, cutting rates as economic data begins to prompt that action, or behind the curve, where the ecomony rolls over rapidly and even the Fed\u0026rsquo;s actions are not enough to halt the economic contraction.\nFinally, we can run an OLS regression to check fit:\ndf = spy_cycle_df #################################### ### Don\u0026#39;t modify below this line ### #################################### # Run OLS regression with statsmodels X = df[\u0026#34;FFR_AnnualizedChange_bps\u0026#34;] y = df[\u0026#34;AnnualizedReturnPct\u0026#34;] X = sm.add_constant(X) model = sm.OLS(y, X).fit() print(model.summary()) print(f\u0026#34;Intercept: {model.params[0]}, Slope: {model.params[1]}\u0026#34;) # Intercept and slope # Calc X and Y values for regression line X_vals = np.linspace(X.min(), X.max(), 100) Y_vals = model.params[0] + model.params[1] * X_vals Which gives us the results of the OLS regression:\nOLS Regression Results =============================================================================== Dep. Variable: AnnualizedReturnPct R-squared: 0.180 Model: OLS Adj. R-squared: 0.089 Method: Least Squares F-statistic: 1.973 Date: Fri, 26 Dec 2025 Prob (F-statistic): 0.194 Time: 10:18:12 Log-Likelihood: -47.173 No. Observations: 11 AIC: 98.35 Df Residuals: 9 BIC: 99.14 Df Model: 1 Covariance Type: nonrobust ============================================================================================ coef std err t P\u0026gt;|t| [0.025 0.975] -------------------------------------------------------------------------------------------- const 12.4404 5.894 2.111 0.064 -0.893 25.774 FFR_AnnualizedChange_bps 0.0430 0.031 1.405 0.194 -0.026 0.112 ============================================================================== Omnibus: 1.065 Durbin-Watson: 3.078 Prob(Omnibus): 0.587 Jarque-Bera (JB): 0.665 Skew: 0.026 Prob(JB): 0.717 Kurtosis: 1.796 Cond. No. 193. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. And then plot the regression line along with the values:\nplot_scatter_regression_ffr_vs_returns( cycle_df=spy_cycle_df, asset_label=\u0026#34;SPY\u0026#34;, index_num=\u0026#34;02\u0026#34;, x_vals=X_vals, y_vals=Y_vals, intercept=model.params[0], slope=model.params[1], ) Which gives us:\nHere we can see the data points for cycles 3 and 7 as mentioned above. Ignoring the data points where the annualized change in FFR is roughly zero (cycles 2, 4, 6, 8, and 10), cycles 1, 5, and 9 fit the economic thesis above, and cycle 11 (which is the current rate cutting cycle), stands as an outlier. Of course, the book is not yet finished for cycle 11, and we could certainly see a bear market in stocks over the next several years.\nBonds (TLT) #Next, we\u0026rsquo;ll run a similar process for long term bonds using TLT as the proxy.\nFirst, we pull data with the following:\n# Set decimal places pandas_set_decimal_places(2) yf_pull_data( base_directory=DATA_DIR, ticker=\u0026#34;TLT\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Exchange_Traded_Funds\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) And then load data with the following:\ntlt = load_data( base_directory=DATA_DIR, ticker=\u0026#34;TLT\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Exchange_Traded_Funds\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, file_format=\u0026#34;pickle\u0026#34;, ) # Filter TLT to date range tlt = tlt[(tlt.index \u0026gt;= pd.to_datetime(start_date)) \u0026amp; (tlt.index \u0026lt;= pd.to_datetime(end_date))] # Resample to monthly frequency tlt_monthly = tlt.resample(\u0026#34;M\u0026#34;).last() tlt_monthly[\u0026#34;Monthly_Return\u0026#34;] = tlt_monthly[\u0026#34;Close\u0026#34;].pct_change() Gives us the following:\nThe columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 252 entries, 2004-11-30 to 2025-10-31 Freq: ME Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Close 252 non-null float64 1 High 252 non-null float64 2 Low 252 non-null float64 3 Open 252 non-null float64 4 Volume 252 non-null int64 5 Monthly_Return 251 non-null float64 dtypes: float64(5), int64(1) memory usage: 13.8 KB The first 5 rows are: | Date | Close | High | Low | Open | Volume | Monthly_Return | |:--------------------|--------:|-------:|------:|-------:|-----------:|-----------------:| | 2004-11-30 00:00:00 | 43.80 | 43.91 | 43.64 | 43.80 | 1754500.00 | nan | | 2004-12-31 00:00:00 | 44.96 | 45.01 | 44.84 | 44.87 | 1056400.00 | 0.03 | | 2005-01-31 00:00:00 | 46.57 | 46.59 | 46.35 | 46.37 | 1313900.00 | 0.04 | | 2005-02-28 00:00:00 | 45.88 | 46.43 | 45.82 | 46.43 | 2797300.00 | -0.01 | | 2005-03-31 00:00:00 | 45.67 | 45.70 | 45.43 | 45.61 | 2410900.00 | -0.00 | The last 5 rows are: | Date | Close | High | Low | Open | Volume | Monthly_Return | |:--------------------|--------:|-------:|------:|-------:|------------:|-----------------:| | 2025-06-30 00:00:00 | 86.00 | 86.18 | 85.37 | 85.62 | 53695200.00 | 0.03 | | 2025-07-31 00:00:00 | 85.02 | 85.50 | 84.94 | 85.22 | 49814100.00 | -0.01 | | 2025-08-31 00:00:00 | 85.03 | 85.28 | 84.87 | 85.18 | 41686400.00 | 0.00 | | 2025-09-30 00:00:00 | 88.08 | 88.74 | 87.92 | 88.37 | 38584000.00 | 0.04 | | 2025-10-31 00:00:00 | 89.30 | 89.66 | 89.21 | 89.56 | 38247300.00 | 0.01 | Next, we can plot the price history before calculating the cycle performance:\nplot_timeseries( price_df=tlt, plot_start_date=start_date, plot_end_date=end_date, plot_columns=[\u0026#34;Close\u0026#34;], title=\u0026#34;TLT Close Price\u0026#34;, x_label=\u0026#34;Date\u0026#34;, x_format=\u0026#34;Year\u0026#34;, y_label=\u0026#34;Price ($)\u0026#34;, y_format=\u0026#34;Decimal\u0026#34;, y_format_decimal_places=0, y_tick_spacing=10, grid=True, legend=False, export_plot=True, plot_file_name=\u0026#34;03_TLT_Price\u0026#34;, ) Next, we will calculate the performance for SPY based on the pre-defined Fed cycles:\ntlt_cycle_df = calc_fed_cycle_asset_performance( fed_cycles=fed_cycles, cycle_labels=cycle_labels, fed_changes=fed_changes, monthly_df=tlt_monthly, ) Which gives us:\n| | Cycle | Start | End | Months | CumulativeReturn | CumulativeReturnPct | AverageMonthlyReturn | AverageMonthlyReturnPct | AnnualizedReturn | AnnualizedReturnPct | Volatility | FedFundsChange | FedFundsChange_bps | FFR_AnnualizedChange | FFR_AnnualizedChange_bps | Label | |---:|:---------|:-----------|:-----------|---------:|-------------------:|----------------------:|-----------------------:|--------------------------:|-------------------:|----------------------:|-------------:|-----------------:|---------------------:|-----------------------:|---------------------------:|:-----------------------------------| | 0 | Cycle 1 | 2004-11-01 | 2006-07-01 | 20 | 0.04 | 4.23 | 0.00 | 0.25 | 0.03 | 2.51 | 0.09 | 0.03 | 331.00 | 0.02 | 198.60 | Cycle 1, 2004-11-01 to 2006-07-01 | | 1 | Cycle 2 | 2006-07-01 | 2007-07-01 | 12 | 0.06 | 5.76 | 0.00 | 0.49 | 0.06 | 5.76 | 0.07 | 0.00 | 2.00 | 0.00 | 2.00 | Cycle 2, 2006-07-01 to 2007-07-01 | | 2 | Cycle 3 | 2007-07-01 | 2008-12-01 | 17 | 0.32 | 32.42 | 0.02 | 1.73 | 0.22 | 21.92 | 0.14 | -0.05 | -510.00 | -0.04 | -360.00 | Cycle 3, 2007-07-01 to 2008-12-01 | | 3 | Cycle 4 | 2008-12-01 | 2015-11-01 | 83 | 0.46 | 45.67 | 0.01 | 0.55 | 0.06 | 5.59 | 0.15 | -0.00 | -4.00 | -0.00 | -0.58 | Cycle 4, 2008-12-01 to 2015-11-01 | | 4 | Cycle 5 | 2015-11-01 | 2019-01-01 | 38 | 0.07 | 7.42 | 0.00 | 0.23 | 0.02 | 2.29 | 0.10 | 0.02 | 228.00 | 0.01 | 72.00 | Cycle 5, 2015-11-01 to 2019-01-01 | | 5 | Cycle 6 | 2019-01-01 | 2019-07-01 | 6 | 0.10 | 10.48 | 0.02 | 1.73 | 0.22 | 22.05 | 0.13 | 0.00 | 0.00 | 0.00 | 0.00 | Cycle 6, 2019-01-01 to 2019-07-01 | | 6 | Cycle 7 | 2019-07-01 | 2020-04-01 | 9 | 0.26 | 26.18 | 0.03 | 2.73 | 0.36 | 36.34 | 0.18 | -0.02 | -235.00 | -0.03 | -313.33 | Cycle 7, 2019-07-01 to 2020-04-01 | | 7 | Cycle 8 | 2020-04-01 | 2022-02-01 | 22 | -0.11 | -11.33 | -0.00 | -0.50 | -0.06 | -6.35 | 0.11 | 0.00 | 3.00 | 0.00 | 1.64 | Cycle 8, 2020-04-01 to 2022-02-01 | | 8 | Cycle 9 | 2022-02-01 | 2023-08-01 | 18 | -0.27 | -26.96 | -0.02 | -1.62 | -0.19 | -18.90 | 0.17 | 0.05 | 525.00 | 0.03 | 350.00 | Cycle 9, 2022-02-01 to 2023-08-01 | | 9 | Cycle 10 | 2023-08-01 | 2024-08-01 | 12 | -0.02 | -1.52 | 0.00 | 0.02 | -0.02 | -1.52 | 0.20 | 0.00 | 0.00 | 0.00 | 0.00 | Cycle 10, 2023-08-01 to 2024-08-01 | | 10 | Cycle 11 | 2024-08-01 | 2025-12-26 | 15 | 0.00 | 0.42 | 0.00 | 0.08 | 0.00 | 0.33 | 0.11 | -0.01 | -145.00 | -0.01 | -116.00 | Cycle 11, 2024-08-01 to 2025-12-26 | This gives us the following data points:\nCycle start date Cycle end date Number of months in the cycle Cumulative return during the cycle (decimal and percent) Average monthly return during the cycle (decimal and percent) Annualized return during the cycle (decimal and percent) Return volatility during the cycle Cumulative change in FFR during the cycle (decimal and basis points) Annualized change in FFR during the cycle (decimal and basis points) From the above DataFrame, we can then plot the cumulative and annualized returns for each cycle in a bar chart. First, the cumulative returns:\nAnd then the annualized returns:\nLet\u0026rsquo;s focus our analysis on the plot comparing the annualized returns for TLT to the change in FFR. We can see that during cycles 3 and 7, the returns were very strong along with a rapid pace in cutting rates. During cycle 9, we see the opposite behavior, where as rates were increased the bond returns were very poor. The question for cycle 11, where bond returns have been essentially flat - is the pace of rate cuts not significant enough to benefit the bond market? Are there other factors at play that are influencing the long term bond returns? Keep in mind that we are also working with 20 year treasuries as well, but we could consider running analysis on investment grade or high yield corporate bonds.\nFinally, we can run an OLS regression with the following code:\ndf = tlt_cycle_df #################################### ### Don\u0026#39;t modify below this line ### #################################### # Run OLS regression with statsmodels X = df[\u0026#34;FFR_AnnualizedChange_bps\u0026#34;] y = df[\u0026#34;AnnualizedReturnPct\u0026#34;] X = sm.add_constant(X) model = sm.OLS(y, X).fit() print(model.summary()) print(f\u0026#34;Intercept: {model.params[0]}, Slope: {model.params[1]}\u0026#34;) # Intercept and slope # Calc X and Y values for regression line X_vals = np.linspace(X.min(), X.max(), 100) Y_vals = model.params[0] + model.params[1] * X_vals Which gives us the results of the OLS regression:\nOLS Regression Results =============================================================================== Dep. Variable: AnnualizedReturnPct R-squared: 0.623 Model: OLS Adj. R-squared: 0.582 Method: Least Squares F-statistic: 14.90 Date: Fri, 26 Dec 2025 Prob (F-statistic): 0.00385 Time: 10:18:26 Log-Likelihood: -39.665 No. Observations: 11 AIC: 83.33 Df Residuals: 9 BIC: 84.13 Df Model: 1 Covariance Type: nonrobust ============================================================================================ coef std err t P\u0026gt;|t| [0.025 0.975] -------------------------------------------------------------------------------------------- const 5.4676 2.978 1.836 0.100 -1.270 12.205 FFR_AnnualizedChange_bps -0.0597 0.015 -3.860 0.004 -0.095 -0.025 ============================================================================== Omnibus: 0.710 Durbin-Watson: 1.219 Prob(Omnibus): 0.701 Jarque-Bera (JB): 0.663 Skew: 0.412 Prob(JB): 0.718 Kurtosis: 2.123 Cond. No. 193. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. And then plot the regression line along with the values:\nplot_scatter_regression_ffr_vs_returns( cycle_df=tlt_cycle_df, asset_label=\u0026#34;TLT\u0026#34;, index_num=\u0026#34;03\u0026#34;, x_vals=X_vals, y_vals=Y_vals, intercept=model.params[0], slope=model.params[1], ) Which gives us:\nThe above plot is intriguing because of how well the OLS regression appears to fit the data. It certainly appears that during rate-cutting cycles, bonds are an asset that performs well.\nGold (GLD) #Lastly, we\u0026rsquo;ll look at the returns on gold, using the GLD ETF as a proxy.\nFirst, we pull data with the following:\n# Set decimal places pandas_set_decimal_places(2) yf_pull_data( base_directory=DATA_DIR, ticker=\u0026#34;GLD\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Exchange_Traded_Funds\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) And then load data with the following:\ngld = load_data( base_directory=DATA_DIR, ticker=\u0026#34;GLD\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Exchange_Traded_Funds\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, file_format=\u0026#34;pickle\u0026#34;, ) # Filter GLD to date range gld = gld[(gld.index \u0026gt;= pd.to_datetime(start_date)) \u0026amp; (gld.index \u0026lt;= pd.to_datetime(end_date))] # Resample to monthly frequency gld_monthly = gld.resample(\u0026#34;M\u0026#34;).last() gld_monthly[\u0026#34;Monthly_Return\u0026#34;] = gld_monthly[\u0026#34;Close\u0026#34;].pct_change() Gives us the following:\nThe columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 252 entries, 2004-11-30 to 2025-10-31 Freq: ME Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Close 252 non-null float64 1 High 252 non-null float64 2 Low 252 non-null float64 3 Open 252 non-null float64 4 Volume 252 non-null int64 5 Monthly_Return 251 non-null float64 dtypes: float64(5), int64(1) memory usage: 13.8 KB The first 5 rows are: | Date | Close | High | Low | Open | Volume | Monthly_Return | |:--------------------|--------:|-------:|------:|-------:|-----------:|-----------------:| | 2004-11-30 00:00:00 | 45.12 | 45.41 | 44.82 | 45.37 | 3857200.00 | nan | | 2004-12-31 00:00:00 | 43.80 | 43.94 | 43.73 | 43.85 | 531600.00 | -0.03 | | 2005-01-31 00:00:00 | 42.22 | 42.30 | 41.96 | 42.21 | 1692400.00 | -0.04 | | 2005-02-28 00:00:00 | 43.53 | 43.74 | 43.52 | 43.68 | 755300.00 | 0.03 | | 2005-03-31 00:00:00 | 42.82 | 42.87 | 42.70 | 42.87 | 1363200.00 | -0.02 | The last 5 rows are: | Date | Close | High | Low | Open | Volume | Monthly_Return | |:--------------------|--------:|-------:|-------:|-------:|------------:|-----------------:| | 2025-06-30 00:00:00 | 304.83 | 304.92 | 301.95 | 302.39 | 8192100.00 | 0.00 | | 2025-07-31 00:00:00 | 302.96 | 304.61 | 302.86 | 304.59 | 8981000.00 | -0.01 | | 2025-08-31 00:00:00 | 318.07 | 318.09 | 314.64 | 314.72 | 15642600.00 | 0.05 | | 2025-09-30 00:00:00 | 355.47 | 355.57 | 350.87 | 351.13 | 13312400.00 | 0.12 | | 2025-10-31 00:00:00 | 368.12 | 370.66 | 365.50 | 370.47 | 11077900.00 | 0.04 | Next, we can plot the price history before calculating the cycle performance:\nplot_timeseries( price_df=gld, plot_start_date=start_date, plot_end_date=end_date, plot_columns=[\u0026#34;Close\u0026#34;], title=\u0026#34;GLD Close Price\u0026#34;, x_label=\u0026#34;Date\u0026#34;, x_format=\u0026#34;Year\u0026#34;, y_label=\u0026#34;Price ($)\u0026#34;, y_format=\u0026#34;Decimal\u0026#34;, y_format_decimal_places=0, y_tick_spacing=25, grid=True, legend=False, export_plot=True, plot_file_name=\u0026#34;04_GLD_Price\u0026#34;, ) Next, we will calculate the performance for SPY based on the pre-defined Fed cycles:\ngld_cycle_df = calc_fed_cycle_asset_performance( fed_cycles=fed_cycles, cycle_labels=cycle_labels, fed_changes=fed_changes, monthly_df=gld_monthly, ) Which gives us:\n| | Cycle | Start | End | Months | CumulativeReturn | CumulativeReturnPct | AverageMonthlyReturn | AverageMonthlyReturnPct | AnnualizedReturn | AnnualizedReturnPct | Volatility | FedFundsChange | FedFundsChange_bps | FFR_AnnualizedChange | FFR_AnnualizedChange_bps | Label | |---:|:---------|:-----------|:-----------|---------:|-------------------:|----------------------:|-----------------------:|--------------------------:|-------------------:|----------------------:|-------------:|-----------------:|---------------------:|-----------------------:|---------------------------:|:-----------------------------------| | 0 | Cycle 1 | 2004-11-01 | 2006-07-01 | 20 | 0.36 | 35.70 | 0.02 | 1.73 | 0.20 | 20.10 | 0.17 | 0.03 | 331.00 | 0.02 | 198.60 | Cycle 1, 2004-11-01 to 2006-07-01 | | 1 | Cycle 2 | 2006-07-01 | 2007-07-01 | 12 | 0.05 | 4.96 | 0.00 | 0.45 | 0.05 | 4.96 | 0.11 | 0.00 | 2.00 | 0.00 | 2.00 | Cycle 2, 2006-07-01 to 2007-07-01 | | 2 | Cycle 3 | 2007-07-01 | 2008-12-01 | 17 | 0.25 | 24.96 | 0.02 | 1.59 | 0.17 | 17.03 | 0.26 | -0.05 | -510.00 | -0.04 | -360.00 | Cycle 3, 2007-07-01 to 2008-12-01 | | 3 | Cycle 4 | 2008-12-01 | 2015-11-01 | 83 | 0.36 | 36.10 | 0.01 | 0.51 | 0.05 | 4.56 | 0.18 | -0.00 | -4.00 | -0.00 | -0.58 | Cycle 4, 2008-12-01 to 2015-11-01 | | 4 | Cycle 5 | 2015-11-01 | 2019-01-01 | 38 | 0.11 | 10.93 | 0.00 | 0.35 | 0.03 | 3.33 | 0.14 | 0.02 | 228.00 | 0.01 | 72.00 | Cycle 5, 2015-11-01 to 2019-01-01 | | 5 | Cycle 6 | 2019-01-01 | 2019-07-01 | 6 | 0.10 | 9.86 | 0.02 | 1.63 | 0.21 | 20.68 | 0.12 | 0.00 | 0.00 | 0.00 | 0.00 | Cycle 6, 2019-01-01 to 2019-07-01 | | 6 | Cycle 7 | 2019-07-01 | 2020-04-01 | 9 | 0.11 | 11.15 | 0.01 | 1.24 | 0.15 | 15.13 | 0.13 | -0.02 | -235.00 | -0.03 | -313.33 | Cycle 7, 2019-07-01 to 2020-04-01 | | 7 | Cycle 8 | 2020-04-01 | 2022-02-01 | 22 | 0.14 | 13.54 | 0.01 | 0.69 | 0.07 | 7.17 | 0.16 | 0.00 | 3.00 | 0.00 | 1.64 | Cycle 8, 2020-04-01 to 2022-02-01 | | 8 | Cycle 9 | 2022-02-01 | 2023-08-01 | 18 | 0.08 | 8.48 | 0.01 | 0.53 | 0.06 | 5.58 | 0.14 | 0.05 | 525.00 | 0.03 | 350.00 | Cycle 9, 2022-02-01 to 2023-08-01 | | 9 | Cycle 10 | 2023-08-01 | 2024-08-01 | 12 | 0.24 | 24.24 | 0.02 | 1.89 | 0.24 | 24.24 | 0.13 | 0.00 | 0.00 | 0.00 | 0.00 | Cycle 10, 2023-08-01 to 2024-08-01 | | 10 | Cycle 11 | 2024-08-01 | 2025-12-26 | 15 | 0.62 | 62.49 | 0.03 | 3.36 | 0.47 | 47.46 | 0.14 | -0.01 | -145.00 | -0.01 | -116.00 | Cycle 11, 2024-08-01 to 2025-12-26 | This gives us the following data points:\nCycle start date Cycle end date Number of months in the cycle Cumulative return during the cycle (decimal and percent) Average monthly return during the cycle (decimal and percent) Annualized return during the cycle (decimal and percent) Return volatility during the cycle Cumulative change in FFR during the cycle (decimal and basis points) Annualized change in FFR during the cycle (decimal and basis points) From the above DataFrame, we can then plot the cumulative and annualized returns for each cycle in a bar chart. First, the cumulative returns:\nAnd then the annualized returns:\nWe see strong returns for gold across several different Fed cycles, so it is difficult to draw any kind of initial conclusion based on the bar charts.\nFinally, we can run an OLS regression with the following code:\ndf = gld_cycle_df #################################### ### Don\u0026#39;t modify below this line ### #################################### # Run OLS regression with statsmodels X = df[\u0026#34;FFR_AnnualizedChange_bps\u0026#34;] y = df[\u0026#34;AnnualizedReturnPct\u0026#34;] X = sm.add_constant(X) model = sm.OLS(y, X).fit() print(model.summary()) print(f\u0026#34;Intercept: {model.params[0]}, Slope: {model.params[1]}\u0026#34;) # Intercept and slope # Calc X and Y values for regression line X_vals = np.linspace(X.min(), X.max(), 100) Y_vals = model.params[0] + model.params[1] * X_vals Which gives us the results of the OLS regression:\nOLS Regression Results =============================================================================== Dep. Variable: AnnualizedReturnPct R-squared: 0.084 Model: OLS Adj. R-squared: -0.018 Method: Least Squares F-statistic: 0.8274 Date: Fri, 26 Dec 2025 Prob (F-statistic): 0.387 Time: 10:18:40 Log-Likelihood: -42.830 No. Observations: 11 AIC: 89.66 Df Residuals: 9 BIC: 90.46 Df Model: 1 Covariance Type: nonrobust ============================================================================================ coef std err t P\u0026gt;|t| [0.025 0.975] -------------------------------------------------------------------------------------------- const 15.1947 3.972 3.826 0.004 6.210 24.179 FFR_AnnualizedChange_bps -0.0187 0.021 -0.910 0.387 -0.065 0.028 ============================================================================== Omnibus: 8.035 Durbin-Watson: 0.915 Prob(Omnibus): 0.018 Jarque-Bera (JB): 3.686 Skew: 1.328 Prob(JB): 0.158 Kurtosis: 3.993 Cond. No. 193. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. And then plot the regression line along with the values:\nplot_scatter_regression_ffr_vs_returns( cycle_df=gld_cycle_df, asset_label=\u0026#34;GLD\u0026#34;, index_num=\u0026#34;04\u0026#34;, x_vals=X_vals, y_vals=Y_vals, intercept=model.params[0], slope=model.params[1], ) Which gives us:\nIt\u0026rsquo;s difficult to draw any strong conclusions with the above plot. Gold has traditionally been considered a hedge for inflation, and while one of the Fed\u0026rsquo;s mandates is to manage inflation, there may not be a conclusion to draw in relationship to the historical returns that gold has exhibited.\nHybrid Portfolio #With the above analysis (somewhat) complete, let\u0026rsquo;s look at the optimal allocation for a portfolio based on the data and the hypythetical historical results.\nRecall the plots for annualized returns vs annualized change in FFR for stocks, bonds, and gold:\nAsset Allocation #We have to be careful with our criteria for when to hold stocks, bonds, or gold, as hindsight bias is certainly possible. So, without overanalyzing the results, let\u0026rsquo;s assume that we hold stocks as the default position, and then hold bonds when the Fed starts cutting rates, and then resume holding stocks when the Fed stops cutting rates. If there is not any change in FFR, then we still hold stocks. That gives us:\nCycle 1: Stocks Cycle 2: Stocks Cycle 3: Bonds Cycle 4: Stocks Cycle 5: Stocks Cycle 6: Stocks Cycle 7: Bonds Cycle 8: Stocks Cycle 9: Stocks Cycle 10: Stocks Cycle 11: Bonds We can then combine the return series based on the above with the following code:\n# Calculate cumulative returns and drawdown for SPY spy_monthly[\u0026#39;Cumulative_Return\u0026#39;] = (1 + spy_monthly[\u0026#39;Monthly_Return\u0026#39;]).cumprod() - 1 spy_monthly[\u0026#39;Cumulative_Return_Plus_One\u0026#39;] = 1 + spy_monthly[\u0026#39;Cumulative_Return\u0026#39;] spy_monthly[\u0026#39;Rolling_Max\u0026#39;] = spy_monthly[\u0026#39;Cumulative_Return_Plus_One\u0026#39;].cummax() spy_monthly[\u0026#39;Drawdown\u0026#39;] = spy_monthly[\u0026#39;Cumulative_Return_Plus_One\u0026#39;] / spy_monthly[\u0026#39;Rolling_Max\u0026#39;] - 1 spy_monthly.drop(columns=[\u0026#39;Cumulative_Return_Plus_One\u0026#39;, \u0026#39;Rolling_Max\u0026#39;], inplace=True) # Calculate cumulative returns and drawdown for TLT tlt_monthly[\u0026#39;Cumulative_Return\u0026#39;] = (1 + tlt_monthly[\u0026#39;Monthly_Return\u0026#39;]).cumprod() - 1 tlt_monthly[\u0026#39;Cumulative_Return_Plus_One\u0026#39;] = 1 + tlt_monthly[\u0026#39;Cumulative_Return\u0026#39;] tlt_monthly[\u0026#39;Rolling_Max\u0026#39;] = tlt_monthly[\u0026#39;Cumulative_Return_Plus_One\u0026#39;].cummax() tlt_monthly[\u0026#39;Drawdown\u0026#39;] = tlt_monthly[\u0026#39;Cumulative_Return_Plus_One\u0026#39;] / tlt_monthly[\u0026#39;Rolling_Max\u0026#39;] - 1 tlt_monthly.drop(columns=[\u0026#39;Cumulative_Return_Plus_One\u0026#39;, \u0026#39;Rolling_Max\u0026#39;], inplace=True) # Isolate the returns for SPY and TLT spy_ret = spy_monthly[\u0026#39;Monthly_Return\u0026#39;] tlt_ret = tlt_monthly[\u0026#39;Monthly_Return\u0026#39;] # Create a blended portfolio based on Fed policy cycles portfolio = ( spy_ret[spy_ret.index \u0026lt;= \u0026#34;2007-07-01\u0026#34;] .combine_first(tlt_ret[(tlt_ret.index \u0026gt;= \u0026#34;2007-07-01\u0026#34;) \u0026amp; (tlt_ret.index \u0026lt;= \u0026#34;2008-12-01\u0026#34;)]) .combine_first(spy_ret[(spy_ret.index \u0026gt; \u0026#34;2008-12-01\u0026#34;) \u0026amp; (spy_ret.index \u0026lt;= \u0026#34;2019-07-01\u0026#34;)]) .combine_first(tlt_ret[(tlt_ret.index \u0026gt;= \u0026#34;2019-07-01\u0026#34;) \u0026amp; (tlt_ret.index \u0026lt;= \u0026#34;2020-04-01\u0026#34;)]) .combine_first(spy_ret[(spy_ret.index \u0026gt; \u0026#34;2020-04-01\u0026#34;) \u0026amp; (spy_ret.index \u0026lt;= \u0026#34;2024-08-01\u0026#34;)]) .combine_first(tlt_ret[tlt_ret.index \u0026gt; \u0026#34;2024-08-01\u0026#34;]) ) # Convert to DataFrame portfolio_monthly = portfolio.to_frame(name=\u0026#34;Portfolio_Monthly_Return\u0026#34;) # Calculate cumulative returns and drawdown for the portfolio portfolio_monthly[\u0026#39;Portfolio_Cumulative_Return\u0026#39;] = (1 + portfolio_monthly[\u0026#39;Portfolio_Monthly_Return\u0026#39;]).cumprod() - 1 portfolio_monthly[\u0026#39;Portfolio_Cumulative_Return_Plus_One\u0026#39;] = 1 + portfolio_monthly[\u0026#39;Portfolio_Cumulative_Return\u0026#39;] portfolio_monthly[\u0026#39;Portfolio_Rolling_Max\u0026#39;] = portfolio_monthly[\u0026#39;Portfolio_Cumulative_Return_Plus_One\u0026#39;].cummax() portfolio_monthly[\u0026#39;Portfolio_Drawdown\u0026#39;] = portfolio_monthly[\u0026#39;Portfolio_Cumulative_Return_Plus_One\u0026#39;] / portfolio_monthly[\u0026#39;Portfolio_Rolling_Max\u0026#39;] - 1 portfolio_monthly.drop(columns=[\u0026#39;Portfolio_Cumulative_Return_Plus_One\u0026#39;, \u0026#39;Portfolio_Rolling_Max\u0026#39;], inplace=True) # Merge \u0026#34;spy_monthly\u0026#34; and \u0026#34;tlt_monthly\u0026#34; into \u0026#34;portfolio_monthly\u0026#34; to compare cumulative returns portfolio_monthly = portfolio_monthly.join( spy_monthly[\u0026#39;Monthly_Return\u0026#39;].rename(\u0026#39;SPY_Monthly_Return\u0026#39;), how=\u0026#39;left\u0026#39; ).join( spy_monthly[\u0026#39;Cumulative_Return\u0026#39;].rename(\u0026#39;SPY_Cumulative_Return\u0026#39;), how=\u0026#39;left\u0026#39; ).join( spy_monthly[\u0026#39;Drawdown\u0026#39;].rename(\u0026#39;SPY_Drawdown\u0026#39;), how=\u0026#39;left\u0026#39; ).join( tlt_monthly[\u0026#39;Monthly_Return\u0026#39;].rename(\u0026#39;TLT_Monthly_Return\u0026#39;), how=\u0026#39;left\u0026#39; ).join( tlt_monthly[\u0026#39;Cumulative_Return\u0026#39;].rename(\u0026#39;TLT_Cumulative_Return\u0026#39;), how=\u0026#39;left\u0026#39; ).join( tlt_monthly[\u0026#39;Drawdown\u0026#39;].rename(\u0026#39;TLT_Drawdown\u0026#39;), how=\u0026#39;left\u0026#39; ) Which gives us:\nThe columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 252 entries, 2004-11-30 to 2025-10-31 Freq: ME Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Portfolio_Monthly_Return 251 non-null float64 1 Portfolio_Cumulative_Return 251 non-null float64 2 Portfolio_Drawdown 251 non-null float64 3 SPY_Monthly_Return 251 non-null float64 4 SPY_Cumulative_Return 251 non-null float64 5 SPY_Drawdown 251 non-null float64 6 TLT_Monthly_Return 251 non-null float64 7 TLT_Cumulative_Return 251 non-null float64 8 TLT_Drawdown 251 non-null float64 dtypes: float64(9) memory usage: 19.7 KB The first 5 rows are: | Date | Portfolio_Monthly_Return | Portfolio_Cumulative_Return | Portfolio_Drawdown | SPY_Monthly_Return | SPY_Cumulative_Return | SPY_Drawdown | TLT_Monthly_Return | TLT_Cumulative_Return | TLT_Drawdown | |:--------------------|---------------------------:|------------------------------:|---------------------:|---------------------:|------------------------:|---------------:|---------------------:|------------------------:|---------------:| | 2004-11-30 00:00:00 | nan | nan | nan | nan | nan | nan | nan | nan | nan | | 2004-12-31 00:00:00 | 0.030 | 0.030 | 0.000 | 0.030 | 0.030 | 0.000 | 0.027 | 0.027 | 0.000 | | 2005-01-31 00:00:00 | -0.022 | 0.007 | -0.022 | -0.022 | 0.007 | -0.022 | 0.036 | 0.063 | 0.000 | | 2005-02-28 00:00:00 | 0.021 | 0.028 | -0.002 | 0.021 | 0.028 | -0.002 | -0.015 | 0.048 | -0.015 | | 2005-03-31 00:00:00 | -0.018 | 0.009 | -0.020 | -0.018 | 0.009 | -0.020 | -0.005 | 0.043 | -0.019 | The last 5 rows are: | Date | Portfolio_Monthly_Return | Portfolio_Cumulative_Return | Portfolio_Drawdown | SPY_Monthly_Return | SPY_Cumulative_Return | SPY_Drawdown | TLT_Monthly_Return | TLT_Cumulative_Return | TLT_Drawdown | |:--------------------|---------------------------:|------------------------------:|---------------------:|---------------------:|------------------------:|---------------:|---------------------:|------------------------:|---------------:| | 2025-06-30 00:00:00 | 0.027 | 19.004 | -0.072 | 0.051 | 6.718 | 0.000 | 0.027 | 0.963 | -0.408 | | 2025-07-31 00:00:00 | -0.011 | 18.776 | -0.082 | 0.023 | 6.896 | 0.000 | -0.011 | 0.941 | -0.415 | | 2025-08-31 00:00:00 | 0.000 | 18.778 | -0.082 | 0.021 | 7.058 | 0.000 | 0.000 | 0.941 | -0.415 | | 2025-09-30 00:00:00 | 0.036 | 19.489 | -0.049 | 0.036 | 7.345 | 0.000 | 0.036 | 1.011 | -0.394 | | 2025-10-31 00:00:00 | 0.014 | 19.772 | -0.036 | 0.024 | 7.544 | 0.000 | 0.014 | 1.039 | -0.385 | Next, we\u0026rsquo;ll look at performance for the assets and portfolio.\nPerformance Statistics #We can then plot the monthly returns:\nAnd cumulative returns:\nAnd drawdowns:\nFinally, we can run the stats on the hybrid portfolio, SPY, and TLT with the following code:\nport_sum_stats = summary_stats( fund_list=[\u0026#34;Portfolio\u0026#34;, \u0026#34;SPY\u0026#34;, \u0026#34;TLT\u0026#34;], df=portfolio_monthly[[\u0026#34;Portfolio_Monthly_Return\u0026#34;]], period=\u0026#34;Monthly\u0026#34;, use_calendar_days=False, excel_export=False, pickle_export=False, output_confirmation=False, ) spy_sum_stats = summary_stats( fund_list=[\u0026#34;Portfolio\u0026#34;, \u0026#34;SPY\u0026#34;, \u0026#34;TLT\u0026#34;], df=portfolio_monthly[[\u0026#34;SPY_Monthly_Return\u0026#34;]], period=\u0026#34;Monthly\u0026#34;, use_calendar_days=False, excel_export=False, pickle_export=False, output_confirmation=False, ) tlt_sum_stats = summary_stats( fund_list=[\u0026#34;Portfolio\u0026#34;, \u0026#34;SPY\u0026#34;, \u0026#34;TLT\u0026#34;], df=portfolio_monthly[[\u0026#34;TLT_Monthly_Return\u0026#34;]], period=\u0026#34;Monthly\u0026#34;, use_calendar_days=False, excel_export=False, pickle_export=False, output_confirmation=False, ) sum_stats = port_sum_stats.combine_first(spy_sum_stats).combine_first(tlt_sum_stats) Which gives us:\n| | Annualized Mean | Annualized Volatility | Annualized Sharpe Ratio | CAGR | Monthly Max Return | Monthly Max Return (Date) | Monthly Min Return | Monthly Min Return (Date) | Max Drawdown | Peak | Trough | Recovery Date | Days to Recover | MAR Ratio | |:-------------------------|------------------:|------------------------:|--------------------------:|-------:|---------------------:|:----------------------------|---------------------:|:----------------------------|---------------:|:--------------------|:--------------------|:--------------------|------------------:|------------:| | Portfolio_Monthly_Return | 0.156 | 0.140 | 1.111 | 0.155 | 0.143 | 2008-11-30 00:00:00 | -0.107 | 2009-02-28 00:00:00 | -0.239 | 2021-12-31 00:00:00 | 2022-09-30 00:00:00 | 2023-12-31 00:00:00 | 457.000 | 0.650 | | SPY_Monthly_Return | 0.114 | 0.148 | 0.769 | 0.108 | 0.127 | 2020-04-30 00:00:00 | -0.165 | 2008-10-31 00:00:00 | -0.508 | 2007-10-31 00:00:00 | 2009-02-28 00:00:00 | 2012-03-31 00:00:00 | 1127.000 | 0.212 | | TLT_Monthly_Return | 0.043 | 0.137 | 0.316 | 0.035 | 0.143 | 2008-11-30 00:00:00 | -0.131 | 2009-01-31 00:00:00 | -0.476 | 2020-07-31 00:00:00 | 2023-10-31 00:00:00 | NaT | nan | 0.072 | Based on the above, our hybrid portfolio outperforms both stocks and bonds, and by a wide margin.\nFuture Investigation #A couple of ideas sound intriguing for future investigation:\nDo investment grade or high yield bonds show a different behavior than the long term US treasury bonds? Does a commodity index (such as GSCI) exhibit differing behavior than gold? How does leverage affect the returns that are observed for the hybrid portfolio, stocks, and bonds? Do other Fed tightening/loosening cycles exhibit the same behavior for returns? References # https://fred.stlouisfed.org/series/FEDFUNDS Code #The Jupyter notebook with the functions and all other code is available here.\nThe HTML export of the jupyter notebook is available here.\nThe PDF export of the jupyter notebook is available here. ","date":"November 29, 2025","permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/asset-class-performance-fed-policy-cycles/","section":"Posts","summary":"How does the performance of stocks, bonds, and other asset classes vary during Fed policy cycles?","title":"Performance Of Various Asset Classes During Fed Policy Cycles"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/python/","section":"Topics","summary":"","title":"Python"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/","section":"Topics","summary":"","title":"Topics"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/yahoo-finance/","section":"Topics","summary":"","title":"Yahoo Finance"},{"content":"Introduction #Similar to the recent post about how I collect and store crypto asset data from Coinbase, the scripts below pull minute, hour, and daily data for equities and ETFs from Polygon.io.\nThe scripts check for an existing data record, and if found then the existing record is updated to include the most recent data. If there is not an existing data record, then the complete historical record from Polygon is pulled and stored.\nPython Functions #Here are the functions needed for this project:\npolygon_fetch_full_history: Fetch full historical data for a given product from Polygon API. polygon_pull_data: Read existing data file, download price data from Polygon, and export data. Function Usage #Polygon Fetch Full History #Here\u0026rsquo;s the docstring with the parameters/variables:\n\u0026#34;\u0026#34;\u0026#34; Fetch full historical data for a given product from Polygon API. Parameters: ----------- client Polygon API client instance. ticker : str Ticker symbol to download. timespan : str Time span for the data (e.g., \u0026#34;minute\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;day\u0026#34;, \u0026#34;week\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;quarter\u0026#34;, \u0026#34;year\u0026#34;). multiplier : int Multiplier for the time span (e.g., 1 for daily data). adjusted : bool If True, return adjusted data; if False, return raw data. full_history_df : pd.DataFrame DataFrame containing the data. current_start : datetime Date for which to start pulling data in datetime format. free_tier : bool If True, then pause to avoid API limits. verbose : bool If True, print detailed information about the data being processed. Returns: -------- full_history_df : pd.DataFrame DataFrame containing the data. \u0026#34;\u0026#34;\u0026#34; This script pulls the full history for a specified asset:\nfrom load_api_keys import load_api_keys from polygon import RESTClient # Load API keys from the environment api_keys = load_api_keys() # Get the environment variable for where data is stored DATA_DIR = config(\u0026#34;DATA_DIR\u0026#34;) # Open client connection client = RESTClient(api_key=api_keys[\u0026#34;POLYGON_KEY\u0026#34;]) # Create an empty DataFrame df = pd.DataFrame({ \u0026#39;Date\u0026#39;: pd.Series(dtype=\u0026#34;datetime64[ns]\u0026#34;), \u0026#39;open\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;high\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;low\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;close\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;volume\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;vwap\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;transactions\u0026#39;: pd.Series(dtype=\u0026#34;int64\u0026#34;), \u0026#39;otc\u0026#39;: pd.Series(dtype=\u0026#34;object\u0026#34;) }) # Example usage - minute df = polygon_fetch_full_history( client=client, ticker=\u0026#34;AMZN\u0026#34;, timespan=\u0026#34;day\u0026#34;, multiplier=1, adjusted=True, full_history_df=df, current_start=datetime(2025, 1, 1), free_tier=True, verbose=True, ) The example above pulls the daily data since 1/1/2025, but can handle data ranges of years because it pulls only a specific number of records at a time as recommended by Polygon (less than 5,000 records per API request), and then combines the records in the dataframe before returning the dataframe.\nHere\u0026rsquo;s the first 5 rows:\n| | Date | open | high | low | close | volume | vwap | transactions | otc | |---:|:--------------------|----------:|----------:|----------:|----------:|---------------:|----------:|---------------:|:------| | 0 | 2025-01-02 05:00:00 | 222.03000 | 225.15000 | 218.19000 | 220.22000 | 33956579.00000 | 221.27450 | 449631 | | | 1 | 2025-01-03 05:00:00 | 222.50500 | 225.36000 | 221.62000 | 224.19000 | 27515606.00000 | 223.70500 | 346976 | | | 2 | 2025-01-06 05:00:00 | 226.78000 | 228.83500 | 224.84000 | 227.61000 | 31849831.00000 | 227.09210 | 410686 | | | 3 | 2025-01-07 05:00:00 | 227.90000 | 228.38100 | 221.46000 | 222.11000 | 28084164.00000 | 223.40330 | 379570 | | | 4 | 2025-01-08 05:00:00 | 223.18500 | 223.52000 | 220.20000 | 222.13000 | 25033292.00000 | 222.04140 | 325539 | | Polygon Pull Data #This script uses the above function to perform the following:\nAttempt to read an existing pickle data file If a data file exists, then pull updated data Otherwise, pull all historical data available for that asset for the past 2 years (using the free tier from Polygon) Store pickle and/or excel files of the data in the specified directories Here\u0026rsquo;s the docstring with the parameters/variables:\n\u0026#34;\u0026#34;\u0026#34; Read existing data file, download price data from Polygon, and export data. Parameters: ----------- base_directory : any Root path to store downloaded data. ticker : str Ticker symbol to download. source : str Name of the data source (e.g., \u0026#39;Polygon\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). start_date : datetime Start date for the data in datetime format. timespan : str Time span for the data (e.g., \u0026#34;minute\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;day\u0026#34;, \u0026#34;week\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;quarter\u0026#34;, \u0026#34;year\u0026#34;). multiplier : int Multiplier for the time span (e.g., 1 for daily data). adjusted : bool If True, return adjusted data; if False, return raw data. force_existing_check : bool If True, force a complete check of the existing data file to verify that there are not any gaps in the data. free_tier : bool If True, then pause to avoid API limits. verbose : bool If True, print detailed information about the data being processed. excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- None \u0026#34;\u0026#34;\u0026#34; Through the base_directory, source, and asset_class variables the script knows where in the local filesystem to look for an existing pickle file and the store the resulting updated pickle and/or excel files:\ncurrent_year = datetime.now().year current_month = datetime.now().month current_day = datetime.now().day # Example usage - daily df = polygon_pull_data( base_directory=DATA_DIR, ticker=\u0026#34;AMZN\u0026#34;, source=\u0026#34;Polygon\u0026#34;, asset_class=\u0026#34;Equities\u0026#34;, start_date=datetime(current_year - 2, current_month, current_day), timespan=\u0026#34;day\u0026#34;, multiplier=1, adjusted=True, force_existing_check=True, free_tier=True, verbose=True, excel_export=True, pickle_export=True, output_confirmation=True, ) Here\u0026rsquo;s the first 5 rows from the output from above:\n| | Date | open | high | low | close | volume | vwap | transactions | otc | |---:|:--------------------|----------:|----------:|----------:|----------:|---------------:|----------:|---------------:|:------| | 0 | 2023-07-28 04:00:00 | 129.69000 | 133.01000 | 129.33000 | 132.21000 | 46269781.00000 | 131.88370 | 413438 | | | 1 | 2023-07-31 04:00:00 | 133.20000 | 133.87000 | 132.38000 | 133.68000 | 41901516.00000 | 133.34100 | 406644 | | | 2 | 2023-08-01 04:00:00 | 133.55000 | 133.69000 | 131.61990 | 131.69000 | 42250989.00000 | 132.24700 | 385743 | | | 3 | 2023-08-02 04:00:00 | 130.15400 | 130.23000 | 126.82000 | 128.21000 | 50988614.00000 | 128.39730 | 532942 | | | 4 | 2023-08-03 04:00:00 | 127.48000 | 129.84000 | 126.41000 | 128.91000 | 90855736.00000 | 131.49410 | 746639 | | We can see that the index is not continuous - but this is not an issue because use of the data would likely need to re-index the data or simply set the date column as the index.\nReferences # https://polygon.io/ https://polygon.io/docs/rest/quickstart Code #The Jupyter notebook with the functions and all other code is available here.\nThe HTML export of the jupyter notebook is available here.\nThe PDF export of the jupyter notebook is available here. ","date":"August 10, 2025","permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/data-pipelining-with-polygon/","section":"Posts","summary":"Acquiring and managing minute, hour, and daily equity and ETF data from Polygon.io.","title":"Data Pipelining With Polygon"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/polygon/","section":"Topics","summary":"","title":"Polygon"},{"content":"Introduction #In this post, I\u0026rsquo;ll cover the implementation of doit to automate the execution of Jupyter notebook files, Python scripts, and building the Hugo static site. Many of the concepts covered below were introduced recently in FINM 32900 - Full-Stack Quantitative Finance. This course emphasized the \u0026ldquo;full stack\u0026rdquo; approach, including the following:\nUse of GitHub Virtual environments Environment variables Use of various data sources (particularly WRDS) Processing/cleaning data GitHub actions Publishing data Restricting access to GitHub hosted sites Motivation #The primary motivation for automation came from several realizations:\nSetting directory variables would avoid any issues with managing where the static files were stored locally I wanted to be able to pull updated data, execute Jupyter notebooks, and update the posts within my Hugo site without a manual intervention and processes I like to include the html and PDF exports of the Jupyter notebooks, which required copying the exports to the \u0026ldquo;Public\u0026rdquo; folder of the website I needed a system to build the \u0026ldquo;index.md\u0026rdquo; files that are present in each post directory, and automatically include Python code and functions (again, without copying/pasting or manual processes) dodo.py #The dodo.py file in the primary directory is referenced by doit and includes all imports, functions, environment variables, etc. as required to execute the desired code. My dodo.py is broken down as follows:\nImports #The inital imports are as follows:\n```python ####################################### ## Import Libraries ####################################### import sys ## Make sure the src folder is in the path sys.path.insert(1, \u0026#34;./src/\u0026#34;) import re import shutil import subprocess import time import yaml from colorama import Fore, Style, init from datetime import datetime from os import environ, getcwd, path from pathlib import Path This first adds the `/src/` subdirectory to the path (required later on), and then imports any other required modules. I prefer to sort all imports alphabetically for easy reference and readability. ### Print PyDoit Text in Green Next, I use the following code to help differentiate the various outputs in the termial when executing `doit`: ```text ```python # Code from lines 29-75 referenced from the UChicago # FINM 32900 - Full-Stack Quantitative Finance course # Credit to Jeremy Bejarano # https://github.com/jmbejara ## Custom reporter: Print PyDoit Text in Green # This is helpful because some tasks write to sterr and pollute the output in # the console. I don\u0026#39;t want to mute this output, because this can sometimes # cause issues when, for example, LaTeX hangs on an error and requires # presses on the keyboard before continuing. However, I want to be able # to easily see the task lines printed by PyDoit. I want them to stand out # from among all the other lines printed to the console. from doit.reporter import ConsoleReporter from settings import config ####################################### ## Slurm Configuration ####################################### try: in_slurm = environ[\u0026#34;SLURM_JOB_ID\u0026#34;] is not None except: in_slurm = False class GreenReporter(ConsoleReporter): def write(self, stuff, **kwargs): doit_mark = stuff.split(\u0026#34; \u0026#34;)[0].ljust(2) task = \u0026#34; \u0026#34;.join(stuff.split(\u0026#34; \u0026#34;)[1:]).strip() + \u0026#34; \u0026#34; output = ( Fore.GREEN + doit_mark + f\u0026#34; {path.basename(getcwd())}: \u0026#34; + task + Style.RESET_ALL ) self.outstream.write(output) if not in_slurm: DOIT_CONFIG = { \u0026#34;reporter\u0026#34;: GreenReporter, # other config here... # \u0026#34;cleanforget\u0026#34;: True, # Doit will forget about tasks that have been cleaned. \u0026#34;backend\u0026#34;: \u0026#34;sqlite3\u0026#34;, \u0026#34;dep_file\u0026#34;: \u0026#34;./.doit-db.sqlite\u0026#34;, } else: DOIT_CONFIG = { \u0026#34;backend\u0026#34;: \u0026#34;sqlite3\u0026#34;, \u0026#34;dep_file\u0026#34;: \u0026#34;./.doit-db.sqlite\u0026#34; } init(autoreset=True) ### Set Directory Variables Next, I establish the variables that reference some of the more important directories and subdirectories in the project: ```text ```python ####################################### ## Set directory variables ####################################### BASE_DIR = config(\u0026#34;BASE_DIR\u0026#34;) CONTENT_DIR = config(\u0026#34;CONTENT_DIR\u0026#34;) POSTS_DIR = config(\u0026#34;POSTS_DIR\u0026#34;) PAGES_DIR = config(\u0026#34;PAGES_DIR\u0026#34;) PUBLIC_DIR = config(\u0026#34;PUBLIC_DIR\u0026#34;) SOURCE_DIR = config(\u0026#34;SOURCE_DIR\u0026#34;) DATA_DIR = config(\u0026#34;DATA_DIR\u0026#34;) DATA_MANUAL_DIR = config(\u0026#34;DATA_MANUAL_DIR\u0026#34;) These directory variables are set from the `settings.py` file in the `/src/` directory. Setting these directory variables allows me to reference them at any point later on in the `dodo.py` file. ### Helper Functions The following are several helper functions that are referenced in the tasks. These are somewhat self explanatory, and are used by the task functions in the next section below: ```python ####################################### ## Helper functions ####################################### def copy_file(origin_path, destination_path, mkdir=True): \u0026#34;\u0026#34;\u0026#34;Create a Python action for copying a file.\u0026#34;\u0026#34;\u0026#34; def _copy_file(): origin = Path(origin_path) dest = Path(destination_path) if mkdir: dest.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(origin, dest) return _copy_file def extract_front_matter(index_path): \u0026#34;\u0026#34;\u0026#34;Extract front matter as a dict from a Hugo index.md file.\u0026#34;\u0026#34;\u0026#34; text = index_path.read_text() match = re.search(r\u0026#34;(?s)^---(.*?)---\u0026#34;, text) if match: return yaml.safe_load(match.group(1)) return {} def notebook_source_hash(notebook_path): \u0026#34;\u0026#34;\u0026#34;Compute a SHA-256 hash of the notebook\u0026#39;s code and markdown cells. This includes all whitespace and comments.\u0026#34;\u0026#34;\u0026#34; import nbformat import hashlib with open(notebook_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: nb = nbformat.read(f, as_version=4) relevant_cells = [ cell[\u0026#34;source\u0026#34;] for cell in nb.cells if cell.cell_type in {\u0026#34;code\u0026#34;, \u0026#34;markdown\u0026#34;} ] full_content = \u0026#34;\\n\u0026#34;.join(relevant_cells) return hashlib.sha256(full_content.encode(\u0026#34;utf-8\u0026#34;)).hexdigest() def clean_pdf_export_pngs(subdir, notebook_name): \u0026#34;\u0026#34;\u0026#34;Remove .png files created by nbconvert during PDF export.\u0026#34;\u0026#34;\u0026#34; pattern = f\u0026#34;{notebook_name}_*_*.png\u0026#34; deleted = False for file in subdir.glob(pattern): print(f\u0026#34; Removing nbconvert temp image: {file}\u0026#34;) file.unlink() deleted = True if not deleted: print(f\u0026#34; No temp PNGs to remove for {notebook_name}\u0026#34;) Tasks #Next, we will look at the individual tasks that are being executed by doit.\nThe config task creates the base directories for the Hugo site:\n####################################### ## PyDoit tasks ####################################### def task_config(): \u0026#34;\u0026#34;\u0026#34;Create empty directories for content, page, post, and public if they don\u0026#39;t exist\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [\u0026#34;ipython ./src/settings.py\u0026#34;], \u0026#34;file_dep\u0026#34;: [\u0026#34;./src/settings.py\u0026#34;], \u0026#34;targets\u0026#34;: [CONTENT_DIR, PAGES_DIR, POSTS_DIR, PUBLIC_DIR], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_list_posts_subdirs function is not really necessary, but was used as an initial starting point for when I began building the dodo.py file:\ndef task_list_posts_subdirs(): \u0026#34;\u0026#34;\u0026#34;Create a list of the subdirectories of the posts directory\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [\u0026#34;python ./src/list_posts_subdirs.py\u0026#34;], \u0026#34;file_dep\u0026#34;: [\u0026#34;./src/settings.py\u0026#34;], # \u0026#34;targets\u0026#34;: [POSTS_DIR], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_run_post_notebooks function performs the following actions:\nFinds all of the \u0026ldquo;post\u0026rdquo; subdirectories In each \u0026ldquo;post\u0026rdquo; directory, it executes the jupyter notebook file (if found) that has the same name as the post The hash of the non-markdown cells in the notebook is also checked, and if the hash has not changed since the last run, then it skips executing the notebook After the notebook is executed (or not), the log is updated with the date and action def task_run_post_notebooks(): \u0026#34;\u0026#34;\u0026#34;Execute notebooks that match their subdirectory names and only when code or markdown content has changed\u0026#34;\u0026#34;\u0026#34; for subdir in POSTS_DIR.iterdir(): if not subdir.is_dir(): continue notebook_path = subdir / f\u0026#34;{subdir.name}.ipynb\u0026#34; if not notebook_path.exists(): continue #  Skip subdirs with no matching notebook hash_file = subdir / f\u0026#34;{subdir.name}.last_source_hash\u0026#34; log_file = subdir / f\u0026#34;{subdir.name}.log\u0026#34; def source_has_changed(path=notebook_path, hash_path=hash_file, log_path=log_file): current_hash = notebook_source_hash(path) timestamp = datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) if hash_path.exists(): old_hash = hash_path.read_text().strip() if current_hash != old_hash: print(f\u0026#34; Change detected in {path.name}\u0026#34;) return False # needs re-run #  No change  log as skipped with log_path.open(\u0026#34;a\u0026#34;) as log: log.write(f\u0026#34;[{timestamp}]  Skipped (no changes): {path.name}\\n\u0026#34;) print(f\u0026#34; No change in hash for {path.name}\u0026#34;) return True #  No previous hash  must run print(f\u0026#34; No previous hash found for {path.name}\u0026#34;) return False def run_and_log(path=notebook_path, hash_path=hash_file, log_path=log_file): start_time = time.time() subprocess.run([ \u0026#34;jupyter\u0026#34;, \u0026#34;nbconvert\u0026#34;, \u0026#34;--execute\u0026#34;, \u0026#34;--to\u0026#34;, \u0026#34;notebook\u0026#34;, \u0026#34;--inplace\u0026#34;, \u0026#34;--log-level=ERROR\u0026#34;, str(path) ], check=True) elapsed = round(time.time() - start_time, 2) new_hash = notebook_source_hash(path) hash_path.write_text(new_hash) print(f\u0026#34; Saved new hash for {path.name}\u0026#34;) timestamp = datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) log_msg = f\u0026#34;[{timestamp}]  Executed {path.name} in {elapsed}s\\n\u0026#34; with log_path.open(\u0026#34;a\u0026#34;) as f: f.write(log_msg) print(log_msg.strip()) yield { \u0026#34;name\u0026#34;: subdir.name, \u0026#34;actions\u0026#34;: [run_and_log], \u0026#34;file_dep\u0026#34;: [notebook_path], \u0026#34;uptodate\u0026#34;: [source_has_changed], \u0026#34;verbosity\u0026#34;: 2, } Next, the task_export_post_notebooks function exports the executed jupyter notebook to both HTML and PDF formats.\ndef task_export_post_notebooks(): \u0026#34;\u0026#34;\u0026#34;Export executed notebooks to HTML and PDF, and clean temp PNGs\u0026#34;\u0026#34;\u0026#34; for subdir in POSTS_DIR.iterdir(): if not subdir.is_dir(): continue notebook_name = subdir.name notebook_path = subdir / f\u0026#34;{notebook_name}.ipynb\u0026#34; html_output = subdir / f\u0026#34;{notebook_name}.html\u0026#34; pdf_output = subdir / f\u0026#34;{notebook_name}.pdf\u0026#34; if not notebook_path.exists(): continue yield { \u0026#34;name\u0026#34;: notebook_name, \u0026#34;actions\u0026#34;: [ f\u0026#34;jupyter nbconvert --to=html --log-level=WARN --output={html_output} {notebook_path}\u0026#34;, f\u0026#34;jupyter nbconvert --to=pdf --log-level=WARN --output={pdf_output} {notebook_path}\u0026#34;, (clean_pdf_export_pngs, [subdir, notebook_name]) ], \u0026#34;file_dep\u0026#34;: [notebook_path], \u0026#34;targets\u0026#34;: [html_output, pdf_output], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_build_post_indices builds each index.md file within each \u0026ldquo;post\u0026rdquo; directory. It looks for an index_temp.md and an index_dep.txt file, which contains the dependencies required to build the index.md file. The dependencies are established within the jupyter notebook for each post, and the index_dep.txt file is also updated when the notebook is executed.\ndef task_build_post_indices(): \u0026#34;\u0026#34;\u0026#34;Run build_index.py in each post subdirectory to generate index.md\u0026#34;\u0026#34;\u0026#34; script_path = SOURCE_DIR / \u0026#34;build_index.py\u0026#34; for subdir in POSTS_DIR.iterdir(): if subdir.is_dir() and (subdir / \u0026#34;index_temp.md\u0026#34;).exists(): def run_script(subdir=subdir): subprocess.run( [\u0026#34;python\u0026#34;, str(script_path)], cwd=subdir, check=True ) yield { \u0026#34;name\u0026#34;: subdir.name, \u0026#34;actions\u0026#34;: [run_script], \u0026#34;file_dep\u0026#34;: [ subdir / \u0026#34;index_temp.md\u0026#34;, subdir / \u0026#34;index_dep.txt\u0026#34;, script_path, ], \u0026#34;targets\u0026#34;: [subdir / \u0026#34;index.md\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } Here\u0026rsquo;s an example of when the index_dep.txt file is generated, and then updated with a markdown file dependency is generated with the export_track_md_deps function:\n# Create file to track markdown dependencies dep_file = Path(\u0026#34;index_dep.txt\u0026#34;) dep_file.write_text(\u0026#34;\u0026#34;) # Copy this \u0026lt;!-- INSERT_01_VIX_Stats_By_Year_HERE --\u0026gt; to index_temp.md export_track_md_deps(dep_file=dep_file, md_filename=\u0026#34;01_VIX_Stats_By_Year.md\u0026#34;, content=vix_stats_by_year.to_markdown(floatfmt=\u0026#34;.2f\u0026#34;)) # Copy this \u0026lt;!-- INSERT_02_VVIX_DF_Info_HERE --\u0026gt; to index_temp.md export_track_md_deps(dep_file=dep_file, md_filename=\u0026#34;02_VVIX_DF_Info.md\u0026#34;, content=df_info_markdown(vix)) # Copy this \u0026lt;!-- INSERT_11_Net_Profit_Percent_HERE --\u0026gt; to index_temp.md export_track_md_deps(dep_file=dep_file, md_filename=\u0026#34;11_Net_Profit_Percent.md\u0026#34;, content=net_profit_percent_str) Moving on, the task_clean_public removes the public directory within the static site. This is necessary to clean out any erroneous files or directories that are changed when the site is rebuilt.\ndef task_clean_public(): \u0026#34;\u0026#34;\u0026#34;Remove the Hugo public directory before rebuilding the site.\u0026#34;\u0026#34;\u0026#34; def remove_public(): if PUBLIC_DIR.exists(): shutil.rmtree(PUBLIC_DIR) print(f\u0026#34; Deleted {PUBLIC_DIR}\u0026#34;) else: print(f\u0026#34; {PUBLIC_DIR} does not exist, nothing to delete.\u0026#34;) return { \u0026#34;actions\u0026#34;: [remove_public], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_build_site builds the Hugo static site.\ndef task_build_site(): \u0026#34;\u0026#34;\u0026#34;Build the Hugo static site\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [\u0026#34;hugo\u0026#34;], \u0026#34;task_dep\u0026#34;: [\u0026#34;clean_public\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_copy_notebook_exports copies the HTML and PDF exports generated above to the public folder. This is necessary due to how Hugo handles HTML and PDF files and excludes those when generating the static site public directories and files.\ndef task_copy_notebook_exports(): \u0026#34;\u0026#34;\u0026#34;Copy notebook HTML exports into the correct Hugo public/ date-based folders\u0026#34;\u0026#34;\u0026#34; for subdir in POSTS_DIR.iterdir(): if subdir.is_dir(): html_file = subdir / f\u0026#34;{subdir.name}.html\u0026#34; index_md = subdir / \u0026#34;index.md\u0026#34; if not html_file.exists() or not index_md.exists(): continue # Extract slug and date from front matter front_matter = extract_front_matter(index_md) slug = front_matter.get(\u0026#34;slug\u0026#34;, subdir.name) date_str = front_matter.get(\u0026#34;date\u0026#34;) if not date_str: continue # Old functionality to format path based on date # # Format path like: public/YYYY/MM/DD/slug/ # date_obj = datetime.fromisoformat(date_str) # public_path = PUBLIC_DIR / f\u0026#34;{date_obj:%Y/%m/%d}\u0026#34; / slug # target_path = public_path / f\u0026#34;{slug}.html\u0026#34; # New functionality to ignore date and just use slug # Format path like: public/posts/slug/ date_obj = datetime.fromisoformat(date_str) public_path = PUBLIC_DIR / \u0026#34;posts\u0026#34; / slug target_path = public_path / f\u0026#34;{slug}.html\u0026#34; def copy_html(src=html_file, dest=target_path): dest.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(src, dest) print(f\u0026#34; Copied {src}  {dest}\u0026#34;) yield { \u0026#34;name\u0026#34;: subdir.name, \u0026#34;actions\u0026#34;: [copy_html], \u0026#34;file_dep\u0026#34;: [html_file, index_md], \u0026#34;targets\u0026#34;: [target_path], \u0026#34;task_dep\u0026#34;: [\u0026#34;build_site\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_create_schwab_callback creates a simple HTML file that will read the authorization code when using oauth with the Schwab API.\ndef task_create_schwab_callback(): \u0026#34;\u0026#34;\u0026#34;Create a Schwab callback URL by creating /public/schwab_callback/index.html and placing the html code in it\u0026#34;\u0026#34;\u0026#34; def create_callback(): callback_path = PUBLIC_DIR / \u0026#34;schwab_callback\u0026#34; / \u0026#34;index.html\u0026#34; callback_path.parent.mkdir(parents=True, exist_ok=True) html = \u0026#34;\u0026#34;\u0026#34;\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Schwab OAuth Code\u0026lt;/title\u0026gt; \u0026lt;script\u0026gt; const params = new URLSearchParams(window.location.search); const code = params.get(\u0026#34;code\u0026#34;); document.write(\u0026#34;\u0026lt;h1\u0026gt;Authorization Code:\u0026lt;/h1\u0026gt;\u0026lt;p\u0026gt;\u0026#34; + code + \u0026#34;\u0026lt;/p\u0026gt;\u0026#34;); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\u0026#34;\u0026#34;\u0026#34; with open(callback_path, \u0026#34;w\u0026#34;) as f: f.write(html) print(f\u0026#34; Created Schwab callback page at {callback_path}\u0026#34;) return { \u0026#34;actions\u0026#34;: [create_callback], \u0026#34;task_dep\u0026#34;: [\u0026#34;copy_notebook_exports\u0026#34;, \u0026#34;clean_public\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } Finally, the task_deploy_site adds any new files, commits the changes, prompts for a message, and pushes the updates to GitHub.\ndef task_deploy_site(): \u0026#34;\u0026#34;\u0026#34;Prompt for a commit message and push to GitHub\u0026#34;\u0026#34;\u0026#34; def commit_and_push(): message = input(\u0026#34;What is the commit message? \u0026#34;) if not message.strip(): print(\u0026#34; Commit message cannot be empty.\u0026#34;) return 1 # signal failure import subprocess subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;add\u0026#34;, \u0026#34;.\u0026#34;], check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;commit\u0026#34;, \u0026#34;-am\u0026#34;, message], check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;push\u0026#34;], check=True) print(\u0026#34; Pushed to GitHub.\u0026#34;) return { \u0026#34;actions\u0026#34;: [commit_and_push], \u0026#34;task_dep\u0026#34;: [\u0026#34;create_schwab_callback\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } As (likely) expected, a good portion of the above code was generated by ChatGPT - somewhere ~ 50%-75%. The balance was generated by myself or modified using the base code provided. Importantly, the general idea of automating the entire process within Hugo and processing the post subdirectories is original (as far as I know).\nFinally, the complete dodo.py and settings.py files are available in the jupyter notebook / HTML / PDF exports linked below.\nExecuting doit #To execute doit, simply run:\n$ doit in the terminal after changing to the high level directory.\nAlternatively, you can list the individual tasks with:\n$ doit list and then execute individually, such as:\n$ doit build_post_indices And finally, doit can be forced to execute all tasks with:\n$ doit --always or an individual task with:\n$ doit --always build_post_indices References # https://pydoit.org/ https://github.com/jmbejara Code #The Jupyter notebook with the functions and all other code is available here.\nThe HTML export of the jupyter notebook is available here.\nThe PDF export of the jupyter notebook is available here. ","date":"June 29, 2025","permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/automating-execution-jupyter-notebook-files-python-scripts-hugo-static-site-generation/","section":"Posts","summary":"A full-stack approach using doit.","title":"Automating Execution of Jupyter Notebook Files, Python Scripts, and Hugo Static Site Generation"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/hugo/","section":"Topics","summary":"","title":"Hugo"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/jupyter-notebook/","section":"Topics","summary":"","title":"Jupyter Notebook"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/bloomberg/","section":"Topics","summary":"","title":"Bloomberg"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/nasdaq-data-link/","section":"Topics","summary":"","title":"Nasdaq Data Link"},{"content":"Introduction #This post intends to provide the code for all of the python functions that I use in my analysis. The goal here is that when writing another post I will simply be able to link to the functions below as opposed to providing the function code in each post.\nFunction Index # bb_clean_data: Takes an Excel export from Bloomberg, removes the miscellaneous headings/rows, and returns a DataFrame. build_index: Reads the index_temp.md markdown file, inserts the markdown dependencies where indicated, and then saves the file as index.md. calc_fed_cycle_asset_performance: Calculates metrics for an asset based on a specified Fed tightening/loosening cycle. calc_vix_trade_pnl: Calculates the profit/loss from VIX options trades. coinbase_fetch_available_products: Fetch available products from Coinbase Exchange API. coinbase_fetch_full_history: Fetch full historical data for a given product from Coinbase Exchange API. coinbase_fetch_historical_candles: Fetch historical candle data for a given product from Coinbase Exchange API. coinbase_pull_data: Update existing record or pull full historical data for a given product from Coinbase Exchange API. df_info: A simple function to display the information about a DataFrame and the first five rows and last five rows. df_info_markdown: Similar to the df_info function above, except that it coverts the output to markdown. export_track_md_deps: Exports various text outputs to markdown files, which are included in the index.md file created when building the site with Hugo. load_data: Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. pandas_set_decimal_places: Set the number of decimal places displayed for floating-point numbers in pandas. plot_timeseries: Plot the price data from a DataFrame for a specified date range and columns. plot_stats: Generate a scatter plot for the mean OHLC prices. plot_vix_with_trades: Plot the VIX daily high and low prices, along with the VIX spikes, and trades. polygon_fetch_full_history: Fetch full historical data for a given product from Polygon API. polygon_pull_data: Read existing data file, download price data from Polygon, and export data. strategy_harry_brown_perm_port: Execute the strategy for the Harry Brown permanent portfolio. summary_stats: Generate summary statistics for a series of returns. yf_pull_data: Download daily price data from Yahoo Finance and export it. Python Functions #bb_clean_data #import os import pandas as pd from IPython.display import display def bb_clean_data( base_directory: str, fund_ticker_name: str, source: str, asset_class: str, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; This function takes an excel export from Bloomberg and removes all excess data leaving date and close columns. Parameters ---------- base_directory : str Root path to store downloaded data. fund : str The fund to clean the data from. source : str Name of the data source (e.g., \u0026#39;Bloomberg\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns ------- df : pd.DataFrame DataFrame containing cleaned data prices. \u0026#34;\u0026#34;\u0026#34; # Set location from where to read existing excel file location = f\u0026#34;{base_directory}/{source}/{asset_class}/Daily/{fund_ticker_name}.xlsx\u0026#34; # Read data from excel try: df = pd.read_excel(location, sheet_name=\u0026#34;Worksheet\u0026#34;, engine=\u0026#34;calamine\u0026#34;) except FileNotFoundError: print(f\u0026#34;File not found...please download the data for {fund_ticker_name}\u0026#34;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace=True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#34;Date\u0026#34;, inplace=True) # Drop the volume column try: df.drop(columns={\u0026#34;PX_VOLUME\u0026#34;}, inplace=True) except KeyError: pass # Rename column df.rename(columns={\u0026#34;PX_LAST\u0026#34;: \u0026#34;Close\u0026#34;}, inplace=True) # Sort by date df.sort_values(by=[\u0026#34;Date\u0026#34;], inplace=True) # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/Daily\u0026#34; os.makedirs(directory, exist_ok=True) # Export to excel if excel_export == True: df.to_excel(f\u0026#34;{directory}/{fund_ticker_name}_Clean.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df.to_pickle(f\u0026#34;{directory}/{fund_ticker_name}_Clean.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;The first and last date of data for {fund_ticker_name} is: \u0026#34;) display(df[:1]) display(df[-1:]) print(f\u0026#34;Bloomberg data cleaning complete for {fund_ticker_name}\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) else: pass return df build_index #from pathlib import Path def build_index() -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Build a Hugo-compatible index.md by combining Markdown fragments. This function reads a template file (`index_temp.md`) and a list of markdown dependencies from `index_dep.txt`. For each entry in the dependency list, it replaces a corresponding placeholder in the template (formatted as \u0026lt;!-- INSERT_\u0026lt;name\u0026gt;_HERE --\u0026gt;) with the content from the markdown file. If a file is missing, the placeholder is replaced with a warning note. Output ------ - Writes the final assembled content to `index.md`. Raises ------ FileNotFoundError: If either `index_temp.md` or `index_dep.txt` does not exist. Example ------- If `index_dep.txt` contains: 01_intro.md 02_analysis.md And `index_temp.md` contains: \u0026lt;!-- INSERT_01_intro_HERE --\u0026gt; \u0026lt;!-- INSERT_02_analysis_HERE --\u0026gt; The resulting `index.md` will include the contents of the respective markdown files in place of their placeholders. \u0026#34;\u0026#34;\u0026#34; temp_index_path = Path(\u0026#34;index_temp.md\u0026#34;) final_index_path = Path(\u0026#34;index.md\u0026#34;) dependencies_path = Path(\u0026#34;index_dep.txt\u0026#34;) # Read the index template if not temp_index_path.exists(): raise FileNotFoundError(\u0026#34;Missing index_temp.md\u0026#34;) temp_index_content = temp_index_path.read_text() # Read dependency list if not dependencies_path.exists(): raise FileNotFoundError(\u0026#34;Missing index_dep.txt\u0026#34;) with dependencies_path.open(\u0026#34;r\u0026#34;) as f: markdown_files = [line.strip() for line in f if line.strip()] # Replace placeholders for each dependency final_index_content = temp_index_content for md_file in markdown_files: placeholder = f\u0026#34;\u0026lt;!-- INSERT_{Path(md_file).stem}_HERE --\u0026gt;\u0026#34; if Path(md_file).exists(): content = Path(md_file).read_text() final_index_content = final_index_content.replace(placeholder, content) else: print( f\u0026#34; Warning: {md_file} not found, skipping placeholder {placeholder}\u0026#34; ) final_index_content = final_index_content.replace( placeholder, f\u0026#34;*{md_file} not found*\u0026#34; ) # Write final index.md final_index_path.write_text(final_index_content) print(\u0026#34; index.md successfully built!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: build_index() calc_fed_cycle_asset_performance #import numpy as np import pandas as pd def calc_fed_cycle_asset_performance( fed_cycles: list, cycle_labels: list, fed_changes: list, monthly_df: pd.DataFrame, ) -\u0026gt; pd.DataFrame: results = [] for (start, end), label in zip(fed_cycles, cycle_labels): start = pd.to_datetime(start) end = pd.to_datetime(end) # Filter TLT returns for the cycle period returns = monthly_df.loc[start:end, \u0026#34;Monthly_Return\u0026#34;] if len(returns) == 0: continue cumulative_return = (1 + returns).prod() - 1 average_return = returns.mean() volatility = returns.std() results.append({ \u0026#34;Cycle\u0026#34;: label, \u0026#34;Start\u0026#34;: start.date(), \u0026#34;End\u0026#34;: end.date(), \u0026#34;Months\u0026#34;: len(returns), \u0026#34;CumulativeReturn\u0026#34;: cumulative_return, \u0026#34;AverageMonthlyReturn\u0026#34;: average_return, \u0026#34;Volatility\u0026#34;: volatility, }) # Convert to DataFrame cycle_df = pd.DataFrame(results) cycle_df[\u0026#34;CumulativeReturnPct\u0026#34;] = 100 * cycle_df[\u0026#34;CumulativeReturn\u0026#34;] cycle_df[\u0026#34;AverageMonthlyReturnPct\u0026#34;] = 100 * cycle_df[\u0026#34;AverageMonthlyReturn\u0026#34;] cycle_df[\u0026#34;AnnualizedReturn\u0026#34;] = (1 + cycle_df[\u0026#34;CumulativeReturn\u0026#34;]) ** (12 / cycle_df[\u0026#34;Months\u0026#34;]) - 1 cycle_df[\u0026#34;AnnualizedReturnPct\u0026#34;] = 100 * cycle_df[\u0026#34;AnnualizedReturn\u0026#34;] # Correct the volatility calculation to annualized volatility cycle_df[\u0026#34;Volatility\u0026#34;] = cycle_df[\u0026#34;Volatility\u0026#34;] * np.sqrt(12) # Re-order columns cycle_df = cycle_df[[ \u0026#34;Cycle\u0026#34;, \u0026#34;Start\u0026#34;, \u0026#34;End\u0026#34;, \u0026#34;Months\u0026#34;, \u0026#34;CumulativeReturn\u0026#34;, \u0026#34;CumulativeReturnPct\u0026#34;, \u0026#34;AverageMonthlyReturn\u0026#34;, \u0026#34;AverageMonthlyReturnPct\u0026#34;, \u0026#34;AnnualizedReturn\u0026#34;, \u0026#34;AnnualizedReturnPct\u0026#34;, \u0026#34;Volatility\u0026#34;, ]] # Merge Fed changes into cycle_df cycle_df[\u0026#34;FedFundsChange\u0026#34;] = fed_changes cycle_df[\u0026#34;FedFundsChange_bps\u0026#34;] = cycle_df[\u0026#34;FedFundsChange\u0026#34;] * 10000 # in basis # Add annualized change in FFR in basis points cycle_df[\u0026#34;FFR_AnnualizedChange\u0026#34;] = (cycle_df[\u0026#34;FedFundsChange\u0026#34;] / cycle_df[\u0026#34;Months\u0026#34;]) * 12 cycle_df[\u0026#34;FFR_AnnualizedChange_bps\u0026#34;] = cycle_df[\u0026#34;FFR_AnnualizedChange\u0026#34;] * 10000 # Convert to basis points cycle_df[\u0026#34;Label\u0026#34;] = cycle_df.apply( lambda row: f\u0026#34;{row[\u0026#39;Cycle\u0026#39;]}, {row[\u0026#39;Start\u0026#39;]} to {row[\u0026#39;End\u0026#39;]}\u0026#34;, axis=1 ) return cycle_df calc_vix_trade_pnl #import pandas as pd def calc_vix_trade_pnl( transaction_df: pd.DataFrame, exp_start_date: str, exp_end_date: str, trade_start_date: str, trade_end_date: str, ) -\u0026gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str, str, str, str]: \u0026#34;\u0026#34;\u0026#34; Calculate the profit and loss (PnL) of trades based on transaction data. Parameters ---------- transaction_df : pd.DataFrame DataFrame containing transaction data. exp_start_date : str Start date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the start of the range for the option expiration date. exp_end_date : str End date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the end of the range for the option expiration date. trade_start_date : str Start date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the start of the range for the trade date. trade_end_date : str End date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the end of the range for the trade date. Returns ------- transactions_data : pd.DataFrame Dataframe containing the transactions for the specified timeframe. closed_trades : pd.DataFrame DataFrame containing the closed trades with realized PnL and percent PnL. open_trades : pd.DataFrame DataFrame containing the open trades. net_PnL_percent_str : str String representation of the net profit percentage. net_PnL_str : str String representation of the net profit and loss in dollars. \u0026#34;\u0026#34;\u0026#34; # If start and end dates for trades and expirations are None, use the entire DataFrame if exp_start_date is None and exp_end_date is None and trade_start_date is None and trade_end_date is None: transactions_data = transaction_df # If both start and end dates for trades and expirations are provided then filter by both else: transactions_data = transaction_df[ (transaction_df[\u0026#39;Exp_Date\u0026#39;] \u0026gt;= exp_start_date) \u0026amp; (transaction_df[\u0026#39;Exp_Date\u0026#39;] \u0026lt;= exp_end_date) \u0026amp; (transaction_df[\u0026#39;Trade_Date\u0026#39;] \u0026gt;= trade_start_date) \u0026amp; (transaction_df[\u0026#39;Trade_Date\u0026#39;] \u0026lt;= trade_end_date) ] # Combine the \u0026#39;Action\u0026#39; and \u0026#39;Symbol\u0026#39; columns to create a unique identifier for each transaction transactions_data[\u0026#39;TradeDate_Action_Symbol_VIX\u0026#39;] = ( transactions_data[\u0026#39;Trade_Date\u0026#39;].astype(str) + \u0026#34;, \u0026#34; + transactions_data[\u0026#39;Action\u0026#39;] + \u0026#34;, \u0026#34; + transactions_data[\u0026#39;Symbol\u0026#39;] + \u0026#34;, VIX = \u0026#34; + transactions_data[\u0026#39;Approx_VIX_Level\u0026#39;].astype(str) ) # Split buys and sells and sum the notional amounts transactions_sells = transactions_data[transactions_data[\u0026#39;Action\u0026#39;] == \u0026#39;Sell to Close\u0026#39;] transactions_sells = transactions_sells.groupby([\u0026#39;Symbol\u0026#39;, \u0026#39;Exp_Date\u0026#39;], as_index=False)[[\u0026#39;Amount\u0026#39;, \u0026#39;Quantity\u0026#39;]].sum() transactions_buys = transactions_data[transactions_data[\u0026#39;Action\u0026#39;] == \u0026#39;Buy to Open\u0026#39;] transactions_buys = transactions_buys.groupby([\u0026#39;Symbol\u0026#39;, \u0026#39;Exp_Date\u0026#39;], as_index=False)[[\u0026#39;Amount\u0026#39;, \u0026#39;Quantity\u0026#39;]].sum() # Merge buys and sells dataframes back together merged_transactions = pd.merge(transactions_buys, transactions_sells, on=[\u0026#39;Symbol\u0026#39;, \u0026#39;Exp_Date\u0026#39;], how=\u0026#39;outer\u0026#39;, suffixes=(\u0026#39;_Buy\u0026#39;, \u0026#39;_Sell\u0026#39;)) merged_transactions = merged_transactions.sort_values(by=[\u0026#39;Exp_Date\u0026#39;], ascending=[True]) merged_transactions = merged_transactions.reset_index(drop=True) # Identify the closed positions merged_transactions[\u0026#39;Closed\u0026#39;] = (~merged_transactions[\u0026#39;Amount_Sell\u0026#39;].isna()) \u0026amp; (~merged_transactions[\u0026#39;Amount_Buy\u0026#39;].isna()) \u0026amp; (merged_transactions[\u0026#39;Quantity_Buy\u0026#39;] == merged_transactions[\u0026#39;Quantity_Sell\u0026#39;]) # Create a new dataframe for closed positions closed_trades = merged_transactions[merged_transactions[\u0026#39;Closed\u0026#39;]] closed_trades = closed_trades.reset_index(drop=True) closed_trades[\u0026#39;Realized_PnL\u0026#39;] = closed_trades[\u0026#39;Amount_Sell\u0026#39;] - closed_trades[\u0026#39;Amount_Buy\u0026#39;] closed_trades[\u0026#39;Percent_PnL\u0026#39;] = closed_trades[\u0026#39;Realized_PnL\u0026#39;] / closed_trades[\u0026#39;Amount_Buy\u0026#39;] closed_trades.drop(columns={\u0026#39;Closed\u0026#39;, \u0026#39;Exp_Date\u0026#39;}, inplace=True) closed_trades[\u0026#39;Quantity_Sell\u0026#39;] = closed_trades[\u0026#39;Quantity_Sell\u0026#39;].astype(int) # Calculate the net % PnL net_PnL_percent = closed_trades[\u0026#39;Realized_PnL\u0026#39;].sum() / closed_trades[\u0026#39;Amount_Buy\u0026#39;].sum() net_PnL_percent_str = f\u0026#34;{round(net_PnL_percent * 100, 2)}%\u0026#34; # Calculate the net $ PnL net_PnL = closed_trades[\u0026#39;Realized_PnL\u0026#39;].sum() net_PnL_str = f\u0026#34;${net_PnL:,.2f}\u0026#34; # Create a new dataframe for open positions open_trades = merged_transactions[~merged_transactions[\u0026#39;Closed\u0026#39;]] open_trades = open_trades.reset_index(drop=True) open_trades.drop(columns={\u0026#39;Closed\u0026#39;, \u0026#39;Amount_Sell\u0026#39;, \u0026#39;Quantity_Sell\u0026#39;, \u0026#39;Exp_Date\u0026#39;}, inplace=True) # Calculate the total market value of opened positions # If start and end dates for trades and expirations are None, use only the closed positions if exp_start_date is None and exp_end_date is None and trade_start_date is None and trade_end_date is None: total_opened_pos_mkt_val = closed_trades[\u0026#39;Amount_Buy\u0026#39;].sum() else: total_opened_pos_mkt_val = closed_trades[\u0026#39;Amount_Buy\u0026#39;].sum() + open_trades[\u0026#39;Amount_Buy\u0026#39;].sum() total_opened_pos_mkt_val_str = f\u0026#34;${total_opened_pos_mkt_val:,.2f}\u0026#34; # Calculate the total market value of closed positions total_closed_pos_mkt_val = closed_trades[\u0026#39;Amount_Sell\u0026#39;].sum() total_closed_pos_mkt_val_str = f\u0026#34;${total_closed_pos_mkt_val:,.2f}\u0026#34; return transactions_data, closed_trades, open_trades, net_PnL_percent_str, net_PnL_str, total_opened_pos_mkt_val_str, total_closed_pos_mkt_val_str coinbase_fetch_available_products #import pandas as pd import requests def coinbase_fetch_available_products( base_currency: str, quote_currency: str, status: str, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Fetch available products from Coinbase Exchange API. Parameters: ----------- base_currency : str, optional Filter products by base currency (e.g., \u0026#39;BTC\u0026#39;). quote_currency : str, optional Filter products by quote currency (e.g., \u0026#39;USD\u0026#39;). status : str, optional Filter products by status (e.g., \u0026#39;online\u0026#39;, \u0026#39;offline\u0026#39;). Returns: -------- pd.DataFrame DataFrame containing available products with their details. \u0026#34;\u0026#34;\u0026#34; url = \u0026#39;https://api.exchange.coinbase.com/products\u0026#39; try: response = requests.get(url, timeout=10) response.raise_for_status() products = response.json() # Convert the list of products into a pandas DataFrame df = pd.DataFrame(products) # Filter by base_currency if provided if base_currency: df = df[df[\u0026#39;base_currency\u0026#39;] == base_currency] # Filter by quote_currency if provided if quote_currency: df = df[df[\u0026#39;quote_currency\u0026#39;] == quote_currency] # Filter by status if provided if status: df = df[df[\u0026#39;status\u0026#39;] == status] # Sort by \u0026#34;id\u0026#34; df = df.sort_values(by=\u0026#39;id\u0026#39;) return df except requests.exceptions.HTTPError as errh: print(f\u0026#34;HTTP Error: {errh}\u0026#34;) except requests.exceptions.ConnectionError as errc: print(f\u0026#34;Error Connecting: {errc}\u0026#34;) except requests.exceptions.Timeout as errt: print(f\u0026#34;Timeout Error: {errt}\u0026#34;) except requests.exceptions.RequestException as err: print(f\u0026#34;Oops: Something Else {err}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # Example usage df = coinbase_fetch_available_products( base_currency=None, quote_currency=\u0026#34;USD\u0026#34;, status=\u0026#34;online\u0026#34;, ) if df is not None: print(df) else: print(\u0026#34;No data returned.\u0026#34;) coinbase_fetch_full_history #import pandas as pd import time from coinbase_fetch_historical_candles import coinbase_fetch_historical_candles from datetime import datetime, timedelta def coinbase_fetch_full_history( product_id: str, start: datetime, end: datetime, granularity: int, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Fetch full historical data for a given product from Coinbase Exchange API. Parameters: ----------- product_id : str The trading pair (e.g., \u0026#39;BTC-USD\u0026#39;). start : datetime Start time in UTC. end : datetime End time in UTC. granularity : int Time slice in seconds (e.g., 3600 for hourly candles). Returns: -------- pd.DataFrame DataFrame containing time, low, high, open, close, volume. \u0026#34;\u0026#34;\u0026#34; all_data = [] current_start = start while current_start \u0026lt; end: current_end = min(current_start + timedelta(seconds=granularity * 300), end) # Fetch max 300 candles per request df = coinbase_fetch_historical_candles(product_id, current_start, current_end, granularity) if df.empty: break all_data.append(df) current_start = df[\u0026#39;time\u0026#39;].iloc[-1] + timedelta(seconds=granularity) time.sleep(0.2) # Small delay to respect rate limits if all_data: full_df = pd.concat(all_data).reset_index(drop=True) return full_df else: return pd.DataFrame() if __name__ == \u0026#34;__main__\u0026#34;: # Example usage df = coinbase_fetch_full_history( product_id=\u0026#34;BTC-USD\u0026#34;, start=datetime(2025, 1, 1), end=datetime(2025, 1, 31), granularity=86_400, ) if df is not None: print(df) else: print(\u0026#34;No data returned.\u0026#34;) coinbase_fetch_historical_candles #import pandas as pd import requests import time from datetime import datetime def coinbase_fetch_historical_candles( product_id: str, start: datetime, end: datetime, granularity: int, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Fetch historical candle data for a given product from Coinbase Exchange API. Parameters: ----------- product_id : str The trading pair (e.g., \u0026#39;BTC-USD\u0026#39;). start : str Start time in UTC. end : str End time in UTC. granularity : int Time slice in seconds (e.g., 60 for minute candles, 3600 for hourly candles, 86,400 for daily candles). Returns: -------- pd.DataFrame DataFrame containing time, low, high, open, close, volume. \u0026#34;\u0026#34;\u0026#34; url = f\u0026#39;https://api.exchange.coinbase.com/products/{product_id}/candles\u0026#39; params = { \u0026#39;start\u0026#39;: start.isoformat(), \u0026#39;end\u0026#39;: end.isoformat(), \u0026#39;granularity\u0026#39;: granularity } max_retries = 5 retry_delay = 1 # initial delay in seconds for attempt in range(max_retries): try: response = requests.get(url, params=params, timeout=10) response.raise_for_status() data = response.json() # Coinbase Exchange API returns data in reverse chronological order data = data[::-1] # Convert to DataFrame df = pd.DataFrame(data, columns=[\u0026#39;time\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;]) df[\u0026#39;time\u0026#39;] = pd.to_datetime(df[\u0026#39;time\u0026#39;], unit=\u0026#39;s\u0026#39;) return df except requests.exceptions.HTTPError as errh: if response.status_code == 429: print(f\u0026#34;Rate limit exceeded. Retrying in {retry_delay} seconds...\u0026#34;) time.sleep(retry_delay) retry_delay *= 2 # Exponential backoff else: print(f\u0026#34;HTTP Error: {errh}\u0026#34;) break except requests.exceptions.ConnectionError as errc: print(f\u0026#34;Error Connecting: {errc}\u0026#34;) time.sleep(retry_delay) retry_delay *= 2 except requests.exceptions.Timeout as errt: print(f\u0026#34;Timeout Error: {errt}\u0026#34;) time.sleep(retry_delay) retry_delay *= 2 except requests.exceptions.RequestException as err: print(f\u0026#34;OOps: Something Else {err}\u0026#34;) break raise Exception(\u0026#34;Failed to fetch data after multiple retries.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # Example usage df = coinbase_fetch_historical_candles( product_id=\u0026#34;BTC-USD\u0026#34;, start=datetime(2025, 1, 1), end=datetime(2025, 1, 2), granularity=3_600, ) if df is not None: print(df) else: print(\u0026#34;No data returned.\u0026#34;) coinbase_pull_data #import calendar import os import pandas as pd from coinbase_fetch_available_products import coinbase_fetch_available_products from coinbase_fetch_full_history import coinbase_fetch_full_history from datetime import datetime, timedelta from settings import config # Get the data directory from the configuration DATA_DIR = config(\u0026#34;DATA_DIR\u0026#34;) def coinbase_pull_data( base_directory, source: str, asset_class: str, excel_export: bool, pickle_export: bool, output_confirmation: bool, base_currency: str, quote_currency: str, granularity: int=3600, # 60=minute, 3600=hourly, 86400=daily status: str=\u0026#39;online\u0026#39;, # default status is \u0026#39;online\u0026#39; start_date: datetime=datetime(2025, 1, 1), # default start date end_date: datetime=datetime.now() - timedelta(days=1), # updates data through 1 day ago due to lag in data availability ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Update existing record or pull full historical data for a given product from Coinbase Exchange API. Parameters: ----------- base_directory Root path to store downloaded data. source : str Name of the data source (e.g., \u0026#39;Nasdaq_Data_Link\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. base_currency : str The base currency (e.g., \u0026#39;BTC\u0026#39;). quote_currency : str The quote currency (e.g., \u0026#39;USD\u0026#39;). status : str, optional Filter products by status (default is \u0026#39;online\u0026#39;). granularity : int Time slice in seconds (e.g., 3600 for hourly candles). start_date : str, optional Start date in UTC (ISO format). end_date : str, optional End date in UTC (ISO format). Returns: -------- None \u0026#34;\u0026#34;\u0026#34; # List of crypto assets filtered_products = coinbase_fetch_available_products(base_currency=base_currency, quote_currency=quote_currency, status=status) filtered_products_list = filtered_products[\u0026#39;id\u0026#39;].tolist() filtered_products_list = sorted(filtered_products_list) if not filtered_products.empty: print(filtered_products[[\u0026#39;id\u0026#39;, \u0026#39;base_currency\u0026#39;, \u0026#39;quote_currency\u0026#39;, \u0026#39;status\u0026#39;]]) print(filtered_products_list) print(len(filtered_products_list)) else: print(\u0026#34;No products found with the specified base and/or quote currencies.\u0026#34;) missing_data = [] omitted_data = [] num_products = len(filtered_products_list) counter = 0 # Loop for updates for product in filtered_products_list: counter+=1 print(f\u0026#34;Updating product {counter} of {num_products}.\u0026#34;) if granularity == 60: time_length = \u0026#34;Minute\u0026#34; elif granularity == 3600: time_length = \u0026#34;Hourly\u0026#34; elif granularity == 86400: time_length = \u0026#34;Daily\u0026#34; else: print(\u0026#34;Error - please confirm timeframe.\u0026#34;) break # Set file location based on parameters file_location = f\u0026#34;{base_directory}/{source}/{asset_class}/{time_length}/{product}.pkl\u0026#34; try: # Attempt to read existing pickle data file ex_data = pd.read_pickle(file_location) ex_data = ex_data.reset_index() print(f\u0026#34;File found...updating the {product} data\u0026#34;) print(\u0026#34;Existing data:\u0026#34;) print(ex_data) # Pull recent data new_data = coinbase_fetch_full_history(product, start_date, end_date, granularity) new_data = new_data.rename(columns={\u0026#39;time\u0026#39;:\u0026#39;Date\u0026#39;}) new_data[\u0026#39;Date\u0026#39;] = new_data[\u0026#39;Date\u0026#39;].dt.tz_localize(None) print(\u0026#34;New data:\u0026#34;) print(new_data) # Combine existing data with recent data full_history_df = pd.concat([ex_data,new_data[new_data[\u0026#39;Date\u0026#39;].isin(ex_data[\u0026#39;Date\u0026#39;]) == False]]) full_history_df = full_history_df.sort_values(by=\u0026#39;Date\u0026#39;) full_history_df[\u0026#39;Date\u0026#39;] = full_history_df[\u0026#39;Date\u0026#39;].dt.tz_localize(None) full_history_df = full_history_df.set_index(\u0026#39;Date\u0026#39;) print(\u0026#34;Combined data:\u0026#34;) print(full_history_df) # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/{time_length}\u0026#34; os.makedirs(directory, exist_ok=True) # Export to excel if excel_export == True: full_history_df.to_excel(f\u0026#34;{directory}/{product}.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: full_history_df.to_pickle(f\u0026#34;{directory}/{product}.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;Data update complete for {time_length} {product}.\u0026#34;) print(\u0026#34;--------------------\u0026#34;) else: pass except FileNotFoundError: # Starting year for fetching initial data starting_year = 2025 # Print error print(f\u0026#34;File not found...downloading the {product} data starting with {starting_year}.\u0026#34;) def get_full_hist(year): try: # Define the start and end dates start_date = datetime(year, 1, 1) # Default start date end_date = datetime.now() - timedelta(days = 1) # Updates data through 1 day ago # Fetch and process the data full_history_df = coinbase_fetch_full_history(product, start_date, end_date, granularity) full_history_df = full_history_df.rename(columns={\u0026#39;time\u0026#39;: \u0026#39;Date\u0026#39;}) full_history_df = full_history_df.sort_values(by=\u0026#39;Date\u0026#39;) # Iterate through rows to see if the value of the asset ever exceeds a specified threshold # Default value for the price threshold is 0 USD # If the price never exceeds this threshold, the asset is omitted from the final list def find_first_close_above_threshold(full_history_df, threshold=0): # Ensure \u0026#39;Date\u0026#39; is the index before proceeding if \u0026#39;Date\u0026#39; in full_history_df.columns: full_history_df.set_index(\u0026#39;Date\u0026#39;, inplace=True) full_history_df.index = full_history_df.index.tz_localize(None) # Iterate through the DataFrame for index, row in full_history_df.iterrows(): if row[\u0026#39;close\u0026#39;] \u0026gt;= threshold: print(f\u0026#34;First occurrence: {index}, close={row[\u0026#39;close\u0026#39;]}\u0026#34;) # Return the filtered DataFrame starting from this row return full_history_df.loc[index:] # If no value meets the condition, return an empty DataFrame print(f\u0026#34;Share price never exceeds {threshold} USD.\u0026#34;) omitted_data.append(product) return None full_history_above_threshold_df = find_first_close_above_threshold(full_history_df, threshold=0) return full_history_above_threshold_df except KeyError: print(f\u0026#34;KeyError: No data available for {product} in {year}. Trying next year...\u0026#34;) next_year = year + 1 # Base case: Stop if the next year exceeds the current year if next_year \u0026gt; datetime.now().year: print(\u0026#34;No more data available for any future years.\u0026#34;) missing_data.append(product) return None # Recursive call for the next year return get_full_hist(year=next_year) # Fetch the full history starting from the given year full_history_df = get_full_hist(year=starting_year) if full_history_df is not None: # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/{time_length}\u0026#34; os.makedirs(directory, exist_ok=True) # Export to excel if excel_export == True: full_history_df.to_excel(f\u0026#34;{directory}/{product}.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: full_history_df.to_pickle(f\u0026#34;{directory}/{product}.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;Initial data fetching completed successfully for {time_length} {product}.\u0026#34;) print(\u0026#34;--------------------\u0026#34;) else: pass else: print(\u0026#34;No data could be fetched for the specified range.\u0026#34;) except Exception as e: print(str(e)) # Remove the cryptocurrencies with missing data from the final list missing_data = sorted(missing_data) print(f\u0026#34;Data missing for: {missing_data}\u0026#34;) for asset in missing_data: try: print(f\u0026#34;Removing {asset} from the list because it is missing data.\u0026#34;) filtered_products_list.remove(asset) except ValueError: print(f\u0026#34;{asset} not in list.\u0026#34;) pass # Remove the cryptocurrencies with share prices that never exceed 1 USD from the final list omitted_data = sorted(omitted_data) print(f\u0026#34;Data omitted due to price for: {omitted_data}\u0026#34;) for asset in omitted_data: try: print(f\u0026#34;Removing {asset} from the list because the share price never exceeds 1 USD.\u0026#34;) filtered_products_list.remove(asset) except ValueError: print(f\u0026#34;{asset} not in list.\u0026#34;) pass # Remove stablecoins from the final list stablecoins_to_remove = [\u0026#39;USDT-USD\u0026#39;, \u0026#39;USDC-USD\u0026#39;, \u0026#39;PAX-USD\u0026#39;, \u0026#39;DAI-USD\u0026#39;, \u0026#39;PYUSD-USD\u0026#39;, \u0026#39;GUSD-USD\u0026#39;] stablecoins_to_remove = sorted(stablecoins_to_remove) print(f\u0026#34;Data for stable coins not to be used: {stablecoins_to_remove}\u0026#34;) for asset in stablecoins_to_remove: try: filtered_products_list.remove(asset) # print(f\u0026#34;Removing {asset} from the list because it is a stablecoin.\u0026#34;) except ValueError: # print(f\u0026#34;{asset} not in list.\u0026#34;) pass # Remove the wrapped coins from the final list wrapped_coins_to_remove = [\u0026#39;WAXL-USD\u0026#39;, \u0026#39;WBTC-USD\u0026#39;] wrapped_coins_to_remove = sorted(wrapped_coins_to_remove) print(f\u0026#34;Data for wrapped coins not to be used: {wrapped_coins_to_remove}\u0026#34;) for asset in wrapped_coins_to_remove: try: filtered_products_list.remove(asset) # print(f\u0026#34;Removing {asset} from the list because it is a wrapped coin.\u0026#34;) except ValueError: # print(f\u0026#34;{asset} not in list.\u0026#34;) pass # Print the final list of token and the length of the list print(f\u0026#34;Final list of tokens: {filtered_products_list}\u0026#34;) print(f\u0026#34;Length of final list of tokens: {len(filtered_products_list)}\u0026#34;) return full_history_df if __name__ == \u0026#34;__main__\u0026#34;: # Example usage to pull all data for each month from 2010 to 2024 for granularity in [60, 3600, 86400]: for year in range(2010, 2025): for month in range(1, 13): print(f\u0026#34;Pulling data for {year}-{month:02d}...\u0026#34;) try: # Get the last day of the month last_day = calendar.monthrange(year, month)[1] coinbase_pull_data( base_directory=DATA_DIR, source=\u0026#34;Coinbase\u0026#34;, asset_class=\u0026#34;Cryptocurrencies\u0026#34;, excel_export=False, pickle_export=True, output_confirmation=True, base_currency=\u0026#34;BTC\u0026#34;, quote_currency=\u0026#34;USD\u0026#34;, granularity=granularity, # 60=minute, 3600=hourly, 86400=daily status=\u0026#39;online\u0026#39;, start_date=datetime(year, month, 1), end_date=datetime(year, month, last_day), ) except Exception as e: print(f\u0026#34;Failed to pull data for {year}-{month:02d}: {e}\u0026#34;) # current_year = datetime.now().year # current_month = datetime.now().month # current_day = datetime.now().day # # Crypto Data # currencies = [\u0026#34;BTC\u0026#34;, \u0026#34;ETH\u0026#34;, \u0026#34;SOL\u0026#34;, \u0026#34;XRP\u0026#34;] # # Iterate through each currency # for cur in currencies: # # Example usage - minute # coinbase_pull_data( # base_directory=DATA_DIR, # source=\u0026#34;Coinbase\u0026#34;, # asset_class=\u0026#34;Cryptocurrencies\u0026#34;, # excel_export=False, # pickle_export=True, # output_confirmation=True, # base_currency=cur, # quote_currency=\u0026#34;USD\u0026#34;, # granularity=60, # 60=minute, 3600=hourly, 86400=daily # status=\u0026#39;online\u0026#39;, # default status is \u0026#39;online\u0026#39; # start_date=datetime(current_year, current_month - 1, 1), # default start date # end_date=datetime.now() - timedelta(days=1), # updates data through 1 day ago due to lag in data availability # ) # # Example usage - hourly # coinbase_pull_data( # base_directory=DATA_DIR, # source=\u0026#34;Coinbase\u0026#34;, # asset_class=\u0026#34;Cryptocurrencies\u0026#34;, # excel_export=True, # pickle_export=True, # output_confirmation=True, # base_currency=cur, # quote_currency=\u0026#34;USD\u0026#34;, # granularity=3600, # 60=minute, 3600=hourly, 86400=daily # status=\u0026#39;online\u0026#39;, # default status is \u0026#39;online\u0026#39; # start_date=datetime(current_year, current_month - 1, 1), # default start date # end_date=datetime.now() - timedelta(days=1), # updates data through 1 day ago due to lag in data availability # ) # # Example usage - daily # coinbase_pull_data( # base_directory=DATA_DIR, # source=\u0026#34;Coinbase\u0026#34;, # asset_class=\u0026#34;Cryptocurrencies\u0026#34;, # excel_export=True, # pickle_export=True, # output_confirmation=True, # base_currency=cur, # quote_currency=\u0026#34;USD\u0026#34;, # granularity=86400, # 60=minute, 3600=hourly, 86400=daily # status=\u0026#39;online\u0026#39;, # default status is \u0026#39;online\u0026#39; # start_date=datetime(current_year, current_month - 1, 1), # default start date # end_date=datetime.now() - timedelta(days=1), # updates data through 1 day ago due to lag in data availability # ) df_info #import pandas as pd from IPython.display import display def df_info( df: pd.DataFrame, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Display summary information about a pandas DataFrame. This function prints: - The DataFrame\u0026#39;s column names, shape, and data types via `df.info()` - The first 5 rows using `df.head()` - The last 5 rows using `df.tail()` It uses `display()` for better output formatting in environments like Jupyter notebooks. Parameters: ----------- df : pd.DataFrame The DataFrame to inspect. Returns: -------- None Example: -------- \u0026gt;\u0026gt;\u0026gt; df_info(my_dataframe) \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;The columns, shape, and data types are:\u0026#34;) print(df.info()) print(\u0026#34;The first 5 rows are:\u0026#34;) display(df.head()) print(\u0026#34;The last 5 rows are:\u0026#34;) display(df.tail()) df_info_markdown #import io import pandas as pd def df_info_markdown( df: pd.DataFrame, decimal_places: int = 2, ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Generate a Markdown-formatted summary of a pandas DataFrame. This function captures and formats the output of `df.info()`, `df.head()`, and `df.tail()` in Markdown for easy inclusion in reports, documentation, or web-based rendering (e.g., Hugo or Jupyter export workflows). Parameters: ----------- df : pd.DataFrame The DataFrame to summarize. Returns: -------- str A string containing the DataFrame\u0026#39;s info, head, and tail formatted in Markdown. Example: -------- \u0026gt;\u0026gt;\u0026gt; print(df_info_markdown(df)) ```text The columns, shape, and data types are: \u0026lt;output from df.info()\u0026gt; ``` The first 5 rows are: | | col1 | col2 | |---|------|------| | 0 | ... | ... | The last 5 rows are: ... \u0026#34;\u0026#34;\u0026#34; buffer = io.StringIO() # Capture df.info() output df.info(buf=buffer) info_str = buffer.getvalue() # Convert head and tail to Markdown head_str = df.head().to_markdown(floatfmt=f\u0026#34;.{decimal_places}f\u0026#34;) tail_str = df.tail().to_markdown(floatfmt=f\u0026#34;.{decimal_places}f\u0026#34;) # markdown = [ # \u0026#34;```text\u0026#34;, # \u0026#34;The columns, shape, and data types are:\\n\u0026#34;, # info_str, # \u0026#34;\\nThe first 5 rows are:\\n\u0026#34;, # head_str, # \u0026#34;\\nThe last 5 rows are:\\n\u0026#34;, # tail_str, # \u0026#34;```\u0026#34; # ] markdown = [ \u0026#34;The columns, shape, and data types are:\\n\u0026#34;, info_str, \u0026#34;\\nThe first 5 rows are:\\n\u0026#34;, head_str, \u0026#34;\\nThe last 5 rows are:\\n\u0026#34;, tail_str ] return \u0026#34;\\n\u0026#34;.join(markdown) export_track_md_deps #from pathlib import Path def export_track_md_deps( dep_file: Path, md_filename: str, content: str, text_or_python: str, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Export Markdown content to a file and track it as a dependency. This function writes the provided content to the specified Markdown file and appends the filename to the given dependency file (typically `index_dep.txt`). This is useful in workflows where Markdown fragments are later assembled into a larger document (e.g., a Hugo `index.md`). Parameters: ----------- dep_file : Path Path to the dependency file that tracks Markdown fragment filenames. md_filename : str The name of the Markdown file to export. content : str The Markdown-formatted content to write to the file. text_or_python : str Indicates whether the content is plain text or Python code for proper formatting. Returns: -------- None Example: -------- \u0026gt;\u0026gt;\u0026gt; export_track_md_deps(Path(\u0026#34;index_dep.txt\u0026#34;), \u0026#34;01_intro.md\u0026#34;, \u0026#34;# Introduction\\n...\u0026#34;)  Exported and tracked: 01_intro.md \u0026#34;\u0026#34;\u0026#34; if text_or_python == \u0026#34;python\u0026#34;: Path(md_filename).write_text(f\u0026#34;```python\\n{content}\\n```\u0026#34;) elif text_or_python == \u0026#34;text\u0026#34;: Path(md_filename).write_text(f\u0026#34;```text\\n{content}\\n```\u0026#34;) else: raise ValueError(\u0026#34;text_or_python must be either \u0026#39;text\u0026#39; or \u0026#39;python\u0026#39;\u0026#34;) with dep_file.open(\u0026#34;a\u0026#34;) as f: f.write(md_filename + \u0026#34;\\n\u0026#34;) print(f\u0026#34; Exported and tracked: {md_filename}\u0026#34;) load_api_keys #import os from dotenv import load_dotenv from pathlib import Path from settings import config # Get the environment variable file path from the configuration ENV_PATH = config(\u0026#34;ENV_PATH\u0026#34;) def load_api_keys( env_path: Path=ENV_PATH ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; Load API keys from a .env file. Parameters: ----------- env_path : Path Path to the .env file. Default is the ENV_PATH from settings. Returns: -------- keys : dict Dictionary of API keys. \u0026#34;\u0026#34;\u0026#34; load_dotenv(dotenv_path=env_path) keys = { \u0026#34;INFURA_KEY\u0026#34;: os.getenv(\u0026#34;INFURA_KEY\u0026#34;), \u0026#34;NASDAQ_DATA_LINK_KEY\u0026#34;: os.getenv(\u0026#34;NASDAQ_DATA_LINK_KEY\u0026#34;), \u0026#34;COINBASE_KEY\u0026#34;: os.getenv(\u0026#34;COINBASE_KEY\u0026#34;), \u0026#34;COINBASE_SECRET\u0026#34;: os.getenv(\u0026#34;COINBASE_SECRET\u0026#34;), \u0026#34;SCHWAB_APP_KEY\u0026#34;: os.getenv(\u0026#34;SCHWAB_APP_KEY\u0026#34;), \u0026#34;SCHWAB_SECRET\u0026#34;: os.getenv(\u0026#34;SCHWAB_SECRET\u0026#34;), \u0026#34;SCHWAB_ACCOUNT_NUMBER_1\u0026#34;: os.getenv(\u0026#34;SCHWAB_ACCOUNT_NUMBER_1\u0026#34;), \u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_1\u0026#34;: os.getenv(\u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_1\u0026#34;), \u0026#34;SCHWAB_ACCOUNT_NUMBER_2\u0026#34;: os.getenv(\u0026#34;SCHWAB_ACCOUNT_NUMBER_2\u0026#34;), \u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_2\u0026#34;: os.getenv(\u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_2\u0026#34;), \u0026#34;SCHWAB_ACCOUNT_NUMBER_3\u0026#34;: os.getenv(\u0026#34;SCHWAB_ACCOUNT_NUMBER_3\u0026#34;), \u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_3\u0026#34;: os.getenv(\u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_3\u0026#34;), \u0026#34;POLYGON_KEY\u0026#34;: os.getenv(\u0026#34;POLYGON_KEY\u0026#34;), } # Raise error if any key is missing for k, v in keys.items(): if not v: raise ValueError(f\u0026#34;Missing environment variable: {k}\u0026#34;) return keys if __name__ == \u0026#34;__main__\u0026#34;: # Example usage api_keys = load_api_keys() print(\u0026#34;API keys loaded successfully.\u0026#34;) load_data #import pandas as pd from pathlib import Path def load_data( base_directory, ticker: str, source: str, asset_class: str, timeframe: str, file_format: str, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. This function attempts to read a file first as a CSV, then as an Excel file (specifically looking for a sheet named \u0026#39;data\u0026#39; and using the \u0026#39;calamine\u0026#39; engine). If both attempts fail, a ValueError is raised. Parameters: ----------- base_directory Root path to read data file. ticker : str Ticker symbol to read. source : str Name of the data source (e.g., \u0026#39;Yahoo\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). timeframe : str Timeframe for the data (e.g., \u0026#39;Daily\u0026#39;, \u0026#39;Month_End\u0026#39;). file_format : str Format of the file to load (\u0026#39;csv\u0026#39;, \u0026#39;excel\u0026#39;, or \u0026#39;pickle\u0026#39;) Returns: -------- pd.DataFrame The loaded data. Raises: ------- ValueError If the file could not be loaded as either CSV or Excel. Example: -------- \u0026gt;\u0026gt;\u0026gt; df = load_data(DATA_DIR, \u0026#34;^VIX\u0026#34;, \u0026#34;Yahoo_Finance\u0026#34;, \u0026#34;Indices\u0026#34;) \u0026#34;\u0026#34;\u0026#34; if file_format == \u0026#34;csv\u0026#34;: csv_path = Path(base_directory) / source / asset_class / timeframe / f\u0026#34;{ticker}.csv\u0026#34; df = pd.read_csv(csv_path) return df elif file_format == \u0026#34;excel\u0026#34;: xlsx_path = Path(base_directory) / source / asset_class / timeframe / f\u0026#34;{ticker}.xlsx\u0026#34; df = pd.read_excel(xlsx_path, sheet_name=\u0026#34;data\u0026#34;, engine=\u0026#34;calamine\u0026#34;) return df elif file_format == \u0026#34;pickle\u0026#34;: pickle_path = Path(base_directory) / source / asset_class / timeframe / f\u0026#34;{ticker}.pkl\u0026#34; df = pd.read_pickle(pickle_path) return df else: raise ValueError(f\u0026#34; Unsupported file format: {file_format}. Please use \u0026#39;csv\u0026#39;, \u0026#39;excel\u0026#39;, or \u0026#39;pickle\u0026#39;.\u0026#34;) pandas_set_decimal_places #import pandas as pd def pandas_set_decimal_places( decimal_places: int, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Set the number of decimal places displayed for floating-point numbers in pandas. Parameters: ---------- decimal_places : int The number of decimal places to display for float values in pandas DataFrames and Series. Returns: -------- None Example: -------- \u0026gt;\u0026gt;\u0026gt; dp(3) \u0026gt;\u0026gt;\u0026gt; pd.DataFrame([1.23456789]) 0 0 1.235 \u0026#34;\u0026#34;\u0026#34; pd.set_option(\u0026#39;display.float_format\u0026#39;, lambda x: f\u0026#39;%.{decimal_places}f\u0026#39; % x) plot_timeseries # plot_stats #import matplotlib.pyplot as plt import pandas as pd from matplotlib.ticker import MultipleLocator def plot_stats( stats_df: pd.DataFrame, plot_columns, title: str, x_label: str, x_rotation: int, x_tick_spacing: int, y_label: str, y_tick_spacing: int, grid: bool, legend: bool, export_plot: bool, plot_file_name: str, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Plot the price data from a DataFrame for a specified date range and columns. Parameters: ----------- stats_df : pd.DataFrame DataFrame containing the price data to plot. plot_columns : str OR list List of columns to plot from the DataFrame. If none, all columns will be plotted. title : str Title of the plot. x_label : str Label for the x-axis. x_rotation : int Rotation angle for the x-axis date labels. x_tick_spacing : int Spacing for the x-axis ticks. y_label : str Label for the y-axis. y_tick_spacing : int Spacing for the y-axis ticks. grid : bool Whether to display a grid on the plot. legend : bool Whether to display a legend on the plot. export_plot : bool Whether to save the figure as a PNG file. plot_file_name : str File name for saving the figure (if save_fig is True). Returns: -------- None \u0026#34;\u0026#34;\u0026#34; # Set plot figure size and background color plt.figure(figsize=(12, 6), facecolor=\u0026#34;#F5F5F5\u0026#34;) # Plot data if plot_columns == \u0026#34;All\u0026#34;: for col in stats_df.columns: plt.scatter( stats_df.index, stats_df[col], label=col, linestyle=\u0026#34;-\u0026#34;, linewidth=1.5 ) else: for col in plot_columns: plt.scatter( stats_df.index, stats_df[col], label=col, linestyle=\u0026#34;-\u0026#34;, linewidth=1.5 ) # Format X axis plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.xlabel(x_label, fontsize=10) plt.xticks(rotation=x_rotation, fontsize=8) # Format Y axis plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(y_label, fontsize=10) plt.yticks(fontsize=8) # Format title, layout, grid, and legend plt.title(title, fontsize=12) plt.tight_layout() if grid == True: plt.grid(True, linestyle=\u0026#34;--\u0026#34;, alpha=0.7) if legend == True: plt.legend(fontsize=9) # Save figure and display plot if export_plot == True: plt.savefig(f\u0026#34;{plot_file_name}.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) # Display the plot plt.show() return None plot_vix_with_trades #import matplotlib.dates as mdates import matplotlib.pyplot as plt import pandas as pd from matplotlib.ticker import MultipleLocator def plot_vix_with_trades( vix_price_df: pd.DataFrame, trades_df: pd.DataFrame, plot_start_date: str, plot_end_date: str, x_tick_spacing: int, y_tick_spacing: int, index_number: str, export_plot: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Plot the VIX daily high and low prices, along with the VIX spikes, and trades. Parameters: ----------- vix_price_df : pd.DataFrame Dataframe containing the VIX price data to plot. trades_df : pd.DataFrame Dataframe containing the trades data. plot_start_date : str Start date for the plot in \u0026#39;YYYY-MM-DD\u0026#39; format. plot_end_date : str End date for the plot in \u0026#39;YYYY-MM-DD\u0026#39; format. index_number : str Index number to be used in the file name of the plot export. export_plot : bool Whether to save the figure as a PNG file. Returns: -------- vix_data : pd.DataFrame Dataframe containing the VIX price data for the specified timeframe. \u0026#34;\u0026#34;\u0026#34; # Create temporary dataframe for the specified date range vix_data = vix_price_df[(vix_price_df.index \u0026gt;= plot_start_date) \u0026amp; (vix_price_df.index \u0026lt;= plot_end_date)] # Set plot figure size and background color plt.figure(figsize=(12, 6), facecolor=\u0026#34;#F5F5F5\u0026#34;) # Plot VIX high and low price data plt.plot(vix_data.index, vix_data[\u0026#39;High\u0026#39;], label=\u0026#39;High\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;steelblue\u0026#39;, linewidth=1) plt.plot(vix_data.index, vix_data[\u0026#39;Low\u0026#39;], label=\u0026#39;Low\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;brown\u0026#39;, linewidth=1) # Plot VIX spikes plt.scatter(vix_data[vix_data[\u0026#39;Spike_SMA\u0026#39;] == True].index, vix_data[vix_data[\u0026#39;Spike_SMA\u0026#39;] == True][\u0026#39;High\u0026#39;], label=\u0026#39;Spike (High \u0026gt; 1.25 * 10 Day High SMA)\u0026#39;, color=\u0026#39;black\u0026#39;, s=20) # Plot trades plt.scatter(trades_df[\u0026#39;Trade_Date\u0026#39;], trades_df[\u0026#39;Approx_VIX_Level\u0026#39;], label=\u0026#39;Trades\u0026#39;, color=\u0026#39;red\u0026#39;, s=20) # Annotate each point in trades_df with the corresponding Action_Symbol for _, row in trades_df.iterrows(): plt.text( row[\u0026#39;Trade_Date\u0026#39;] + pd.Timedelta(days=1), row[\u0026#39;Approx_VIX_Level\u0026#39;] + 0.1, row[\u0026#39;TradeDate_Action_Symbol_VIX\u0026#39;], fontsize=9 ) # Format X axis plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=x_tick_spacing)) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#34;%Y-%m-%d\u0026#34;)) plt.xlabel(\u0026#34;Date\u0026#34;, fontsize=10) plt.xticks(rotation=45, fontsize=8) # Format Y axis plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(\u0026#34;VIX\u0026#34;, fontsize=10) plt.yticks(fontsize=8) # Format title, layout, grid, and legend plt.title(f\u0026#34;CBOE Volatility Index (VIX), VIX Spikes, Trades, {plot_start_date} - {plot_end_date}\u0026#34;, fontsize=12) plt.tight_layout() plt.grid(True, linestyle=\u0026#39;--\u0026#39;, alpha=0.7) plt.legend(fontsize=9) # Save figure and display plot if export_plot == True: # plt.savefig(f\u0026#34;{index_number}_VIX_Spike_Trades_{plot_start_date}_{plot_end_date}.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) plt.savefig(f\u0026#34;{index_number}_VIX_Spike_Trades.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) # Display the plot plt.show() return vix_data polygon_fetch_full_history #import pandas as pd import time from datetime import datetime, timedelta from load_api_keys import load_api_keys from polygon import RESTClient from settings import config # Load API keys from the environment api_keys = load_api_keys() # Get the environment variable for where data is stored DATA_DIR = config(\u0026#34;DATA_DIR\u0026#34;) def polygon_fetch_full_history( client, ticker: str, timespan: str, multiplier: int, adjusted: bool, existing_history_df: pd.DataFrame, current_start: datetime, free_tier: bool, verbose: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Fetch full historical data for a given product from Polygon API. Parameters: ----------- client Polygon API client instance. ticker : str Ticker symbol to download. timespan : str Time span for the data (e.g., \u0026#34;minute\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;day\u0026#34;, \u0026#34;week\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;quarter\u0026#34;, \u0026#34;year\u0026#34;). multiplier : int Multiplier for the time span (e.g., 1 for daily data). adjusted : bool If True, return adjusted data; if False, return raw data. full_history_df : pd.DataFrame DataFrame containing the data. current_start : datetime Date for which to start pulling data in datetime format. free_tier : bool If True, then pause to avoid API limits. verbose : bool If True, print detailed information about the data being processed. Returns: -------- full_history_df : pd.DataFrame DataFrame containing the data. \u0026#34;\u0026#34;\u0026#34; # Copy DataFrame full_history_df = existing_history_df.copy() if timespan == \u0026#34;minute\u0026#34;: time_delta = 15 time_overlap = 1 elif timespan == \u0026#34;hour\u0026#34;: time_delta = 15 time_overlap = 1 elif timespan == \u0026#34;day\u0026#34;: time_delta = 180 time_overlap = 1 else: raise Exception(f\u0026#34;Invalid {timespan}.\u0026#34;) new_data_last_date = None new_date_last_date_check = None while current_start \u0026lt; datetime.now(): # Offset end date by time_delta current_end = current_start + timedelta(days=time_delta) if verbose == True: print(f\u0026#34;Pulling {timespan} data for {current_start} thru {current_end} for {ticker}...\\n\u0026#34;) try: # Pull new data aggs = client.get_aggs( ticker=ticker, timespan=timespan, multiplier=multiplier, from_=current_start, to=current_end, adjusted=adjusted, sort=\u0026#34;asc\u0026#34;, limit=5000, ) # if len(aggs) == 0: # raise Exception(f\u0026#34;No data is available for {ticker} for {current_start} thru {current_end}. Please attempt different dates.\u0026#34;) # Convert to DataFrame new_data = pd.DataFrame([bar.__dict__ for bar in aggs]) new_data[\u0026#34;timestamp\u0026#34;] = pd.to_datetime(new_data[\u0026#34;timestamp\u0026#34;], unit=\u0026#34;ms\u0026#34;) new_data = new_data.rename(columns = {\u0026#39;timestamp\u0026#39;:\u0026#39;Date\u0026#39;}) new_data = new_data[[\u0026#39;Date\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;vwap\u0026#39;, \u0026#39;transactions\u0026#39;, \u0026#39;otc\u0026#39;]] new_data = new_data.sort_values(by=\u0026#39;Date\u0026#39;, ascending=True) # Enforce dtypes to match full_history_df new_data = new_data.astype(full_history_df.dtypes.to_dict()) # (Optional) reorder columns to match schema # new_data = new_data[full_history_df.columns] # Find last date in new_data new_data_last_date = new_data[\u0026#39;Date\u0026#39;].max() if verbose == True: print(\u0026#34;New data:\u0026#34;) print(new_data) # No longer necessary to check for 5000 rows of data # Check if new data contains 5000 rows # if len(new_data) == 5000: # raise Exception(f\u0026#34;New data for {ticker} contains 5000 rows, indicating potential issues with data completeness or API limits.\u0026#34;) # If full_history_df length is not 0, check to confirm that data overlaps to verify that there were not any splits in the data # if not full_history_df.empty: # if not full_history_df[\u0026#39;Date\u0026#39;].isin(new_data[\u0026#39;Date\u0026#39;]).any(): # raise Exception(f\u0026#34;New data does not overlap with existing data.\u0026#34;) if not full_history_df.empty: # Columns present in both frames common_cols = list(full_history_df.columns.intersection(new_data.columns)) if not common_cols: raise Exception(\u0026#34;No common columns to compare.\u0026#34;) # (Optional) de-duplicate to speed up the merge # full_dedup = full_history_df[common_cols].drop_duplicates() # new_dedup = new_data[common_cols].drop_duplicates() price_cols = [\u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;] # Inner join on every shared column = exact row matches # overlap = full_dedup.merge(new_dedup, on=common_cols, how=\u0026#34;inner\u0026#34;) # overlap = full_dedup.merge(new_dedup, on=price_cols, how=\u0026#34;inner\u0026#34;) overlap = full_history_df.merge(new_data, on=price_cols, how=\u0026#34;inner\u0026#34;) if overlap.empty: raise Exception(f\u0026#34;New data does not overlap with existing data (full-row check).\u0026#34;) # Combine existing data with recent data, drop duplicates, sort values, reset index full_history_df = pd.concat([full_history_df, new_data]) full_history_df = full_history_df.drop_duplicates(subset=\u0026#34;Date\u0026#34;, keep=\u0026#34;last\u0026#34;) full_history_df = full_history_df.sort_values(by=\u0026#39;Date\u0026#39;,ascending=True) full_history_df = full_history_df.reset_index(drop=True) if verbose == True: print(\u0026#34;Combined data:\u0026#34;) print(full_history_df) except KeyError as e: print(f\u0026#34;No data is available for {ticker} from {current_start} thru {current_end}.\u0026#34;) user_choice = input( f\u0026#34;Press Enter to continue, or type \u0026#39;q\u0026#39; to quit: \u0026#34; ) if user_choice.lower() == \u0026#34;q\u0026#34;: print(f\u0026#34;Aborting operation to update {ticker} {timespan} data.\u0026#34;) break # break out of the while loop else: print(f\u0026#34;Trying next timeframe for {ticker} {timespan} data.\u0026#34;) # Set last_data_date to current_end because we know data was not available # up until current_end new_data_last_date = current_end pass except Exception as e: print(f\u0026#34;Failed to pull {timespan} data for {current_start} thru {current_end} for {ticker}: {e}\u0026#34;) raise # Re-raise the original exception # Break out of loop if data is up-to-date (or close to being up-to-date because it is # possible that entire range was not pulled due to the way API handles hour data # from minute data) if current_end \u0026gt; datetime.now(): break else: # Edge case, if the last date for new_date is exactly time_overlap\u0026#39;s duration # past current_start if new_date_last_date_check == new_data_last_date: current_start = current_end - timedelta(days=time_overlap) new_date_last_date_check = new_data_last_date else: current_start = new_data_last_date - timedelta(days=time_overlap) new_date_last_date_check = new_data_last_date # Code below is likely not necessary # # Overlap with existing data to capture all data but check to see if # # current_end is a Sunday and if so ensure overlap covers a trading day # if current_end.weekday() == 6: # current_start = last_data_date - timedelta(days=(time_overlap+1)) # else: # current_start = last_data_date - timedelta(days=time_overlap) # Check for free tier and if so then pause for 12 seconds to avoid hitting API rate limits if free_tier == True: if verbose == True: print(f\u0026#34;Sleeping for 12 seconds to avoid hitting API rate limits...\\n\u0026#34;) time.sleep(12) # Return the DataFrame containing the full history return full_history_df if __name__ == \u0026#34;__main__\u0026#34;: current_year = datetime.now().year current_month = datetime.now().month current_day = datetime.now().day # Open client connection client = RESTClient(api_key=api_keys[\u0026#34;POLYGON_KEY\u0026#34;]) # Create an empty DataFrame df = pd.DataFrame({ \u0026#39;Date\u0026#39;: pd.Series(dtype=\u0026#34;datetime64[ns]\u0026#34;), \u0026#39;open\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;high\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;low\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;close\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;volume\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;vwap\u0026#39;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#39;transactions\u0026#39;: pd.Series(dtype=\u0026#34;int64\u0026#34;), \u0026#39;otc\u0026#39;: pd.Series(dtype=\u0026#34;object\u0026#34;) }) # Example usage - minute df = polygon_fetch_full_history( client=client, ticker=\u0026#34;TQQQ\u0026#34;, timespan=\u0026#34;hour\u0026#34;, multiplier=1, adjusted=True, existing_history_df=df, current_start=datetime(current_year - 2, current_month, current_day), free_tier=True, verbose=True, ) polygon_pull_data #import os import pandas as pd import time from datetime import datetime, timedelta from IPython.display import display from load_api_keys import load_api_keys from polygon import RESTClient from polygon_fetch_full_history import polygon_fetch_full_history from settings import config # Load API keys from the environment api_keys = load_api_keys() # Get the environment variable for where data is stored DATA_DIR = config(\u0026#34;DATA_DIR\u0026#34;) def polygon_pull_data( base_directory, ticker: str, source: str, asset_class: str, start_date: datetime, timespan: str, multiplier: int, adjusted: bool, force_existing_check: bool, free_tier: bool, verbose: bool, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Read existing data file, download price data from Polygon, and export data. Parameters: ----------- base_directory : any Root path to store downloaded data. ticker : str Ticker symbol to download. source : str Name of the data source (e.g., \u0026#39;Polygon\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). start_date : datetime Start date for the data in datetime format. timespan : str Time span for the data (e.g., \u0026#34;minute\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;day\u0026#34;, \u0026#34;week\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;quarter\u0026#34;, \u0026#34;year\u0026#34;). multiplier : int Multiplier for the time span (e.g., 1 for daily data). adjusted : bool If True, return adjusted data; if False, return raw data. force_existing_check : bool If True, force a complete check of the existing data file to verify that there are not any gaps in the data. free_tier : bool If True, then pause to avoid API limits. verbose : bool If True, print detailed information about the data being processed. excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- pd.DataFrame DataFrame containing the updated price data. \u0026#34;\u0026#34;\u0026#34; # Open client connection client = RESTClient(api_key=api_keys[\u0026#34;POLYGON_KEY\u0026#34;]) # Set file location based on parameters file_location = f\u0026#34;{base_directory}/{source}/{asset_class}/{timespan}/{ticker}.pkl\u0026#34; # Create list of acceptable timespans acceptable_timespans = [\u0026#34;minute\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;day\u0026#34;] if timespan not in acceptable_timespans: raise Exception(f\u0026#34;Invalid timespan: {timespan}. Acceptable timespans are: {acceptable_timespans}.\u0026#34;) # if timespan == \u0026#34;minute\u0026#34;: # time_delta = 15 # time_overlap = 1 # elif timespan == \u0026#34;hour\u0026#34;: # time_delta = 15 # time_overlap = 1 # elif timespan == \u0026#34;day\u0026#34;: # time_delta = 180 # time_overlap = 1 # else: # raise Exception(f\u0026#34;Invalid {timespan}.\u0026#34;) try: # Attempt to read existing pickle data file existing_history_df = pd.read_pickle(file_location) # Reset index if \u0026#39;Date\u0026#39; is column is the index if \u0026#34;Date\u0026#34; not in existing_history_df.columns: existing_history_df = existing_history_df.reset_index() print(f\u0026#34;File found...updating the {ticker} {timespan} data.\u0026#34;) if verbose == True: print(\u0026#34;Existing data:\u0026#34;) print(existing_history_df) # Find last date in existing data last_data_date = existing_history_df[\u0026#34;Date\u0026#34;].max() print(f\u0026#34;Last date in existing data: {last_data_date}\u0026#34;) # Find the number of starting rows starting_rows = len(existing_history_df) print(f\u0026#34;Number of rows in existing data: {starting_rows}\u0026#34;) # Overlap with existing data to capture all data # current_start = last_data_date - timedelta(days=time_overlap) current_start = last_data_date - timedelta(days=1) except FileNotFoundError: # Print error print(f\u0026#34;File not found...downloading the {ticker} {timespan} data.\u0026#34;) # Create an empty DataFrame existing_history_df = pd.DataFrame( { \u0026#34;Date\u0026#34;: pd.Series(dtype=\u0026#34;datetime64[ns]\u0026#34;), \u0026#34;open\u0026#34;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#34;high\u0026#34;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#34;low\u0026#34;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#34;close\u0026#34;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#34;volume\u0026#34;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#34;vwap\u0026#34;: pd.Series(dtype=\u0026#34;float64\u0026#34;), \u0026#34;transactions\u0026#34;: pd.Series(dtype=\u0026#34;int64\u0026#34;), \u0026#34;otc\u0026#34;: pd.Series(dtype=\u0026#34;object\u0026#34;), } ) # Set the number of starting rows to be 0 starting_rows = 0 # Set current date to start date current_start = start_date # Check for force_existing_check flag if force_existing_check == True: print(\u0026#34;Forcing check of existing data...\u0026#34;) current_start = start_date full_history_df = polygon_fetch_full_history( client=client, ticker=ticker, timespan=timespan, multiplier=multiplier, adjusted=adjusted, existing_history_df=existing_history_df, current_start=current_start, free_tier=free_tier, verbose=verbose, ) # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/{timespan}\u0026#34; os.makedirs(directory, exist_ok=True) # Export to Excel if excel_export == True: print(f\u0026#34;Exporting {ticker} {timespan} data to Excel...\u0026#34;) full_history_df.to_excel(f\u0026#34;{directory}/{ticker}.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) # Export to Pickle if pickle_export == True: print(f\u0026#34;Exporting {ticker} {timespan} data to Pickle...\u0026#34;) full_history_df.to_pickle(f\u0026#34;{directory}/{ticker}.pkl\u0026#34;) total_rows = len(full_history_df) # Output confirmation if output_confirmation == True: print(f\u0026#34;The first and last date of {timespan} data for {ticker} is: \u0026#34;) display(full_history_df[:1]) display(full_history_df[-1:]) print(f\u0026#34;Number of rows after data update: {total_rows}\u0026#34;) if starting_rows: print(f\u0026#34;Number of rows added during update: {total_rows - starting_rows}\u0026#34;) print(f\u0026#34;Polygon data complete for {ticker} {timespan} data.\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) return full_history_df if __name__ == \u0026#34;__main__\u0026#34;: # Get current year, month, day current_year = datetime.now().year current_month = datetime.now().month current_day = datetime.now().day # Set global variables GLOBAL_FREE_TIER = True # Stock Data equities = [\u0026#34;AMZN\u0026#34;, \u0026#34;AAPL\u0026#34;] # Iterate through each stock for stock in equities: # Example usage - minute polygon_pull_data( base_directory=DATA_DIR, ticker=stock, source=\u0026#34;Polygon\u0026#34;, asset_class=\u0026#34;Equities\u0026#34;, start_date=datetime(current_year - 2, current_month, current_day), timespan=\u0026#34;minute\u0026#34;, multiplier=1, adjusted=True, force_existing_check=False, free_tier=GLOBAL_FREE_TIER, verbose=False, excel_export=True, pickle_export=True, output_confirmation=True, ) if GLOBAL_FREE_TIER == True: time.sleep(12) else: pass # Example usage - hourly polygon_pull_data( base_directory=DATA_DIR, ticker=stock, source=\u0026#34;Polygon\u0026#34;, asset_class=\u0026#34;Equities\u0026#34;, start_date=datetime(current_year - 2, current_month, current_day), timespan=\u0026#34;hour\u0026#34;, multiplier=1, adjusted=True, force_existing_check=False, free_tier=GLOBAL_FREE_TIER, verbose=False, excel_export=True, pickle_export=True, output_confirmation=True, ) if GLOBAL_FREE_TIER == True: time.sleep(12) else: pass # Example usage - daily polygon_pull_data( base_directory=DATA_DIR, ticker=stock, source=\u0026#34;Polygon\u0026#34;, asset_class=\u0026#34;Equities\u0026#34;, start_date=datetime(current_year - 2, current_month, current_day), timespan=\u0026#34;day\u0026#34;, multiplier=1, adjusted=True, force_existing_check=False, free_tier=GLOBAL_FREE_TIER, verbose=False, excel_export=True, pickle_export=True, output_confirmation=True, ) if GLOBAL_FREE_TIER == True: time.sleep(12) else: pass strategy_harry_brown_perm_port #import pandas as pd def strategy_harry_brown_perm_port( fund_list: str, starting_cash: int, cash_contrib: int, close_prices_df: pd.DataFrame, rebal_month: int, rebal_day: int, rebal_per_high: float, rebal_per_low: float, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Execute the re-balance strategy based on specified criteria. Parameters: ----------- fund_list (str): List of funds for data to be combined from. Funds are strings in the form \u0026#34;BTC-USD\u0026#34;. starting_cash (int): Starting investment balance. cash_contrib (int): Cash contribution to be made daily. close_prices_df (pd.DataFrame): DataFrame containing date and close prices for all funds to be included. rebal_month (int): Month for annual rebalance. rebal_day (int): Day for annual rebalance. rebal_per_high (float): High percentage for rebalance. rebal_per_low (float): Low percentage for rebalance. excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df (pd.DataFrame): DataFrame containing strategy data for all funds to be included. Also dumps the df to excel for reference later. \u0026#34;\u0026#34;\u0026#34; num_funds = len(fund_list) df = close_prices_df.copy() df.reset_index(inplace = True) # Date to be used for annual rebalance target_month = rebal_month target_day = rebal_day # Create a dataframe with dates from the specific month rebal_date = df[df[\u0026#39;Date\u0026#39;].dt.month == target_month] # Specify the date or the next closest rebal_date = rebal_date[rebal_date[\u0026#39;Date\u0026#39;].dt.day \u0026gt;= target_day] # Group by year and take the first entry for each year rebal_dates_by_year = rebal_date.groupby(rebal_date[\u0026#39;Date\u0026#39;].dt.year).first().reset_index(drop=True) \u0026#39;\u0026#39;\u0026#39; Column order for the dataframe: df[fund + \u0026#34;_BA_Shares\u0026#34;] df[fund + \u0026#34;_BA_$_Invested\u0026#34;] df[fund + \u0026#34;_BA_Port_%\u0026#34;] df[\u0026#39;Total_BA_$_Invested\u0026#39;] df[\u0026#39;Contribution\u0026#39;] df[\u0026#39;Rebalance\u0026#39;] df[fund + \u0026#34;_AA_Shares\u0026#34;] df[fund + \u0026#34;_AA_$_Invested\u0026#34;] df[fund + \u0026#34;_AA_Port_%\u0026#34;] df[\u0026#39;Total_AA_$_Invested\u0026#39;] \u0026#39;\u0026#39;\u0026#39; # Calculate the columns and initial values for before action (BA) shares, $ invested, and port % for fund in fund_list: df[fund + \u0026#34;_BA_Shares\u0026#34;] = starting_cash / num_funds / df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_BA_$_Invested\u0026#34;] = df[fund + \u0026#34;_BA_Shares\u0026#34;] * df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_BA_Port_%\u0026#34;] = 0.25 # Set column values initially df[\u0026#39;Total_BA_$_Invested\u0026#39;] = starting_cash df[\u0026#39;Contribution\u0026#39;] = cash_contrib df[\u0026#39;Rebalance\u0026#39;] = \u0026#34;No\u0026#34; # Set columns and values initially for after action (AA) shares, $ invested, and port % for fund in fund_list: df[fund + \u0026#34;_AA_Shares\u0026#34;] = starting_cash / num_funds / df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_AA_$_Invested\u0026#34;] = df[fund + \u0026#34;_AA_Shares\u0026#34;] * df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_AA_Port_%\u0026#34;] = 0.25 # Set column value for after action (AA) total $ invested df[\u0026#39;Total_AA_$_Invested\u0026#39;] = starting_cash # Iterate through the dataframe and execute the strategy for index, row in df.iterrows(): # Ensure there\u0026#39;s a previous row to reference by checking the index value if index \u0026gt; 0: # Initialize variable Total_BA_Invested = 0 # Calculate before action (BA) shares and $ invested values for fund in fund_list: df.at[index, fund + \u0026#34;_BA_Shares\u0026#34;] = df.at[index - 1, fund + \u0026#34;_AA_Shares\u0026#34;] df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] = df.at[index, fund + \u0026#34;_BA_Shares\u0026#34;] * row[fund + \u0026#34;_Close\u0026#34;] # Sum the asset values to find the total Total_BA_Invested = Total_BA_Invested + df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] # Calculate before action (BA) port % values for fund in fund_list: df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] = df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] / Total_BA_Invested # Set column for before action (BA) total $ invested df.at[index, \u0026#39;Total_BA_$_Invested\u0026#39;] = Total_BA_Invested # Initialize variables rebalance = \u0026#34;No\u0026#34; date = row[\u0026#39;Date\u0026#39;] # Check for a specific date annually # Simple if statement to check if date_to_check is in jan_28_or_after_each_year if date in rebal_dates_by_year[\u0026#39;Date\u0026#39;].values: rebalance = \u0026#34;Yes\u0026#34; else: pass # Check to see if any asset has portfolio percentage of greater than 35% or less than 15% and if so set variable for fund in fund_list: if df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] \u0026gt; rebal_per_high or df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] \u0026lt; rebal_per_low: rebalance = \u0026#34;Yes\u0026#34; else: pass # If rebalance is required, rebalance back to 25% for each asset, else just divide contribution evenly across assets if rebalance == \u0026#34;Yes\u0026#34;: df.at[index, \u0026#39;Rebalance\u0026#39;] = rebalance for fund in fund_list: df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] = (Total_BA_Invested + df.at[index, \u0026#39;Contribution\u0026#39;]) * 0.25 else: df.at[index, \u0026#39;Rebalance\u0026#39;] = rebalance for fund in fund_list: df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] = df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] + df.at[index, \u0026#39;Contribution\u0026#39;] * 0.25 # Initialize variable Total_AA_Invested = 0 # Set column values for after action (AA) shares and port % for fund in fund_list: df.at[index, fund + \u0026#34;_AA_Shares\u0026#34;] = df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] / row[fund + \u0026#34;_Close\u0026#34;] # Sum the asset values to find the total Total_AA_Invested = Total_AA_Invested + df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] # Calculate after action (AA) port % values for fund in fund_list: df.at[index, fund + \u0026#34;_AA_Port_%\u0026#34;] = df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] / Total_AA_Invested # Set column for after action (AA) total $ invested df.at[index, \u0026#39;Total_AA_$_Invested\u0026#39;] = Total_AA_Invested # If this is the first row else: pass df[\u0026#39;Return\u0026#39;] = df[\u0026#39;Total_AA_$_Invested\u0026#39;].pct_change() df[\u0026#39;Cumulative_Return\u0026#39;] = (1 + df[\u0026#39;Return\u0026#39;]).cumprod() plan_name = \u0026#39;_\u0026#39;.join(fund_list) # Export to excel if excel_export == True: df.to_excel(f\u0026#34;{plan_name}_Strategy.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df.to_pickle(f\u0026#34;{plan_name}_Strategy.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;Strategy complete for {plan_name}\u0026#34;) else: pass return df summary_stats #import pandas as pd import numpy as np def summary_stats( fund_list: list[str], df: pd.DataFrame, period: str, use_calendar_days: bool, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Calculate summary statistics for the given fund list and return data. Parameters: ----------- fund_list (str): List of funds. This is used below in the excel/pickle export but not in the analysis.. Funds are strings in the form \u0026#34;BTC-USD\u0026#34;. df (pd.DataFrame): Dataframe with return data. Assumes returns are in decimal format (e.g., 0.05 for 5%), and assumes there is only 1 column. period (str): Period for which to calculate statistics. Options are \u0026#34;Monthly\u0026#34;, \u0026#34;Weekly\u0026#34;, \u0026#34;Daily\u0026#34;. use_calendar_days (bool): If True, use calendar days for calculations. If False, use trading days. excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df_stats (pd.DataFrame): pd.DataFrame: DataFrame containing various portfolio statistics. \u0026#34;\u0026#34;\u0026#34; # Get the period in proper format period = period.strip().capitalize() # Map base timeframes period_to_timeframe = { \u0026#34;Monthly\u0026#34;: 12, \u0026#34;Weekly\u0026#34;: 52, \u0026#34;Daily\u0026#34;: 365 if use_calendar_days else 252, } try: timeframe = period_to_timeframe[period] except KeyError: raise ValueError(f\u0026#34;Invalid period: {period}. Must be one of {list(period_to_timeframe.keys())}\u0026#34;) df_stats = pd.DataFrame(df.mean(axis=0) * timeframe) # annualized df_stats.columns = [\u0026#39;Annualized Mean\u0026#39;] df_stats[\u0026#39;Annualized Volatility\u0026#39;] = df.std() * np.sqrt(timeframe) # annualized df_stats[\u0026#39;Annualized Sharpe Ratio\u0026#39;] = df_stats[\u0026#39;Annualized Mean\u0026#39;] / df_stats[\u0026#39;Annualized Volatility\u0026#39;] df_cagr = (1 + df[df.columns[0]]).cumprod() cagr = (df_cagr.iloc[-1] / 1) ** ( 1 / (len(df_cagr) / timeframe)) - 1 df_stats[\u0026#39;CAGR\u0026#39;] = cagr df_stats[f\u0026#39;{period} Max Return\u0026#39;] = df.max() df_stats[f\u0026#39;{period} Max Return (Date)\u0026#39;] = df.idxmax().values[0] df_stats[f\u0026#39;{period} Min Return\u0026#39;] = df.min() df_stats[f\u0026#39;{period} Min Return (Date)\u0026#39;] = df.idxmin().values[0] wealth_index = 1000 * (1 + df).cumprod() previous_peaks = wealth_index.cummax() drawdowns = (wealth_index - previous_peaks) / previous_peaks df_stats[\u0026#39;Max Drawdown\u0026#39;] = drawdowns.min() df_stats[\u0026#39;Peak\u0026#39;] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns] df_stats[\u0026#39;Trough\u0026#39;] = drawdowns.idxmin() recovery_date = [] for col in wealth_index.columns: prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max() recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T recovery_date.append(recovery_wealth[recovery_wealth[col] \u0026gt;= prev_max].index.min()) df_stats[\u0026#39;Recovery Date\u0026#39;] = recovery_date df_stats[\u0026#39;Days to Recover\u0026#39;] = (df_stats[\u0026#39;Recovery Date\u0026#39;] - df_stats[\u0026#39;Trough\u0026#39;]).dt.days df_stats[\u0026#39;MAR Ratio\u0026#39;] = df_stats[\u0026#39;CAGR\u0026#39;] / -df_stats[\u0026#39;Max Drawdown\u0026#39;] plan_name = \u0026#39;_\u0026#39;.join(fund_list) # Export to excel if excel_export == True: df_stats.to_excel(f\u0026#34;{plan_name}_Summary_Stats.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df_stats.to_pickle(f\u0026#34;{plan_name}_Summary_Stats.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;Summary stats complete for {plan_name}\u0026#34;) else: pass return df_stats yf_pull_data #import os import pandas as pd import yfinance as yf from IPython.display import display def yf_pull_data( base_directory, ticker: str, source: str, asset_class: str, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Download daily price data from Yahoo Finance and export it. Parameters: ----------- base_directory Root path to store downloaded data. ticker : str Ticker symbol to download. source : str Name of the data source (e.g., \u0026#39;Yahoo\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df : pd.DataFrame DataFrame containing the downloaded data. \u0026#34;\u0026#34;\u0026#34; # Download data from YF df = yf.download(ticker, start=\u0026#34;1900-01-01\u0026#34;) # Drop the column level with the ticker symbol df.columns = df.columns.droplevel(1) # Reset index df = df.reset_index() # Remove the \u0026#34;Price\u0026#34; header from the index df.columns.name = None # Reset date column df[\u0026#39;Date\u0026#39;] = df[\u0026#39;Date\u0026#39;].dt.tz_localize(None) # Set \u0026#39;Date\u0026#39; column as index df = df.set_index(\u0026#39;Date\u0026#39;, drop=True) # Drop data from last day because it\u0026#39;s not accrate until end of day df = df.drop(df.index[-1]) # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/Daily\u0026#34; os.makedirs(directory, exist_ok=True) # Export to excel if excel_export == True: df.to_excel(f\u0026#34;{directory}/{ticker}.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df.to_pickle(f\u0026#34;{directory}/{ticker}.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;The first and last date of data for {ticker} is: \u0026#34;) display(df[:1]) display(df[-1:]) print(f\u0026#34;Yahoo Finance data complete for {ticker}\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) else: pass return df References #None\nCode #The Jupyter notebook with the functions and all other code is available here.\nThe HTML export of the jupyter notebook is available here.\nThe PDF export of the jupyter notebook is available here. ","date":"February 2, 2025","permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/reusable-extensible-python-functions-financial-data-analysis/","section":"Posts","summary":"A list of common functions used for data acquisition, cleaning, analysis, etc.","title":"Reusable And Extensible Python Functions For Financial Data Analysis"},{"content":"Introduction #In this tutorial, we will write a python function that imports an excel export from Bloomberg, removes ancillary rows and columns, and leaves the data in a format where it can then be used in time series analysis.\nExample of a Bloomberg excel export #We will use the SPX index data in this example. Exporting the data from Bloomberg using the excel Bloomberg add-on yields data in the following format:\nData modifications #The above format isn\u0026rsquo;t horrible, but we want to perform the following modifications:\nRemove the first six rows of the data Convert the 7th row to become column headings Rename column 2 to \u0026ldquo;Close\u0026rdquo; to represent the closing price Remove column 3, as we are not concerned about volume Export to excel and make the name of the excel worksheet \u0026ldquo;data\u0026rdquo; Assumptions #The remainder of this tutorial assumes the following:\nYour excel file is named \u0026ldquo;SPX_Index.xlsx\u0026rdquo; The worksheet in the excel file is named \u0026ldquo;Worksheet\u0026rdquo; You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data #The following function will perform the modifications mentioned above:\n# This function takes an excel export from Bloomberg and # removes all excess data leaving date and close columns # Imports import pandas as pd # Function definition def bb_data_updater(fund): # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports #First, we need to import pandas:\nimport pandas as pd Import excel data file #Then import the excel file as a pandas dataframe:\n# File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) Running:\ndf.head(10) Gives us:\nSet column headings #Next, set the column heading:\n# Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] Now, running:\ndf.head(10) Gives us:\nRemove index heading #Next, remove the column heading from the index column:\n# Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) Note: The axis=1 argument here specifies the column index.\nNow, running:\ndf.head(10) Gives us:\nDrop rows #Next, we want to remove the first 6 rows that have unneeded data:\n# Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) Note: When dropping rows, the range to drop begins with row 0 and continues up to - but not including - row 6.\nNow, running:\ndf.head(10) Gives us:\nSet index #Next, we want to set the date column as the index:\n# Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nDrop the \u0026ldquo;PX_VOLUME\u0026rdquo; column #Next, we want to drop the volume column:\n# Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass For some data records, the volume column does not exist. Therefore, we try, and if it fails with a KeyError, then we assume the \u0026ldquo;PX_VOLUME\u0026rdquo; column does not exist, and just pass to move on.\nNow, running:\ndf.head(10) Gives us:\nRename the \u0026ldquo;PX_LAST\u0026rdquo; column #Next, we want to rename the \u0026ldquo;PX_LAST\u0026rdquo; column as \u0026ldquo;Close\u0026rdquo;:\n# Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) Now, running:\ndf.head(10) Gives us:\nSort data #Next, we want to sort the data starting with the oldest date:\n# Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) Now, running:\ndf.head(10) Gives us:\nExport data #Next, we want to export the data to an excel file, for easy viewing and reference later:\n# Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation #Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n# Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences # https://www.bloomberg.com/professional/support/software-updates/ ","date":"November 15, 2023","permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/cleaning-bloomberg-excel-export/","section":"Posts","summary":"A python function to clean and format an excel data export from Bloomberg.","title":"Cleaning A Bloomberg Data Excel Export"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/arch-linux/","section":"Topics","summary":"","title":"Arch Linux"},{"content":"Introduction #This is the basic framework that I use to install Arch Linux, with a few changes catered to the Lenovo ThinkPad E15 Gen 2. I have found that this is a decent mid range laptop, excellent linux compatibility, great keyboard, and overall provides a good value.\nGetting started #This tutorial assumes the following:\nYou are booting from a USB drive with the Arch install ISO. Wireless or wired network is detected and drivers are configured automatically. You want drive encrytion on your root partition, but not on your boot/efi/swap partitions. Verify UEFI boot mode #The following command should show directory without error:\n# ls /sys/firmware/efi/efivars Configure wireless network #The following command will drop you into the iwd daemon:\n# iwctl From there:\n# device list # station *device* scan # station *device* get-networks # station *device* connect *SSID* Verify internet connectivity ## ping archlinux.org Update system clock ## timedatectl set-ntp true # timedatectl status Disks, partition table \u0026amp; partitions #The following assumes that your NVME drive is found as /dev/nvme0n1. Partitions will then be /dev/nvme0n1p1 and so on.\nWipe disk #List disks:\n# fdisk -l Wipe all file system records:\n# wipefs -a /dev/nvme0n1 Create new partition table #Open nvme0n1 with gdisk:\n# gdisk /dev/nvme0n1 Create GPT partition table with option \u0026ldquo;o\u0026rdquo;.\nCreate EFI partition #Create new EFI partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #1 Default starting sector +550M Change partition type to EFI System (ef00) Create boot partition #Create new boot partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #2 Default starting sector +550M Leave default type of 8300 Create swap partition #The old rule of thumb used to be that a swap partition should be the same size as the amount of memory in the system, but given the typical amount of memory in modern systems this is obviously no longer necessary. For my system with 16 or 32 GB of memory, a swap of 8 GB is rarely even used.\nCreate new Swap partition w/ 8GB with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #3 Default starting sector +8G Change to linux swap (8200) Create root partition #Create new root partition w/ remaining disk space with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #4 Default starting sector Remaining space Linux LUKS type 8309 And then exit gdisk.\nWrite file systems #EFI partition #Write file system to new EFI System partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p1 # mkfs.fat -F32 /dev/nvme0n1p1 Boot partition #Then boot partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p2 # mkfs.ext2 /dev/nvme0n1p2 Root partition #Prepare root partition w/ LUKS:\n# cryptsetup -y -v luksFormat --type luks2 /dev/nvme0n1p4 # cryptsetup luksDump /dev/nvme0n1p4 # cryptsetup open /dev/nvme0n1p4 archcryptroot # mkfs.ext4 /dev/mapper/archcryptroot # mount /dev/mapper/archcryptroot /mnt I use archcryptroot for the name of my encrypted volume, but change as necessary.\nSwap partition #Then swap:\n# mkswap /dev/nvme0n1p3 # swapon /dev/nvme0n1p3 Create mount points ## mkdir /mnt/boot # mount /dev/nvme0n1p2 /mnt/boot # mkdir /mnt/boot/efi # mount /dev/nvme0n1p1 /mnt/boot/efi System install #Install base packages ## pacstrap /mnt base base-devel linux linux-firmware grub-efi-x86_64 efibootmgr Generate fstab ## genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Enter new system ## arch-chroot /mnt /bin/bash Set clock ## ln -sf /usr/share/zoneinfo/America/Chicago /etc/localtime # hwclock systohc Generate locale #In /etc/locale.gen uncomment only: en_US.UTF-8 UTF-8\n# locale-gen In /etc/locale.conf, you should only have this line: LANG=en_US.UTF-8\n# nano /etc/locale.conf Set hostname \u0026amp; update hosts ## echo linuxmachine \u0026gt; /etc/hostname Update /etc/hosts with the following:\n127.0.0.1 localhost ::1 localhost 127.0.1.1 linuxmachine.localdomain linuxmachine Set root password ## passwd Update /etc/mkinitcpio.conf \u0026amp; generate initrd image #Edit /etc/mkinitcpio.conf with the following:\nHOOKS=(base udev autodetect modconf block keymap encrypt resume filesystems keyboard fsck) Then run:\n# mkinitcpio -p linux Install grub ## grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=ArchLinux Edit /etc/default/grub so it includes a statement like this:\nGRUB_CMDLINE_LINUX=\u0026quot;cryptdevice=/dev/nvme0n1p4:archcryptroot resume=/dev/nvme0n1p3\u0026quot; Generate final grub configuration:\n# grub-mkconfig -o /boot/grub/grub.cfg Exit \u0026amp; reboot ## exit # umount -R /mnt # swapoff -a # reboot To be continued.\n","date":"September 29, 2023","permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/posts/arch-linux-install/","section":"Posts","summary":"A guide to Arch Linux install with customizations for the  Lenovo ThinkPad E15 Gen 2.","title":"Arch Linux Install"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/linux/","section":"Topics","summary":"","title":"Linux"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/topics/thinkpad-e15-gen-2/","section":"Topics","summary":"","title":"Thinkpad E15 Gen 2"},{"content":" Jared Szajkowski An Engineer Studying Quant Finance Profile #Jared is a seasoned civil engineer with extensive experience in infrastructure design and construction, specifically within the heavy highway and telecommunications sectors. With a strong foundation in field operations, contract administration, and project cost forecasting, Jared has consistently demonstrated the ability to drive projects to success through aggressive problem-solving and meticulous planning.\nAfter spending the first part of his career in engineering, Jared decided to follow his passion for financial markets by enrolling in the Financial Mathematics graduate program at the University of Chicago in the fall of 2022. This rigorous program, known for its emphasis on quantitative analysis, stochastic processes, and financial modeling, has equipped Jared with advanced skills in data-driven decision-making, risk management, and derivative pricing. He will complete the program in the spring of 2026.\nJared aims to leverage his prior professional experience, analytical skills, business acumen, and attention to detail to a new career in the financial services industry.\nUndergraduate Education #Jared graduated from the Illinois Institue of Technology with a Bachelor of Science (BS) in Civil Engineering.\nGraduate Education #Jared has completed the following graduate courses in the Univeristy of Chicago Financial Mathematics program:\nFINM 34000 - Probability and Stochastic Processes FINM 36700 - Portfolio and Risk Management FINM 32500 - Computing for Finance in Python FINM 32600 - Computing for Finance in C++ FINM 32700 - Advanced Computing for Finance FINM 33000 - Option Pricing FINM 33150 - Quantitative Trading Strategy Example coursework: The final group project consisted of developing a trading strategy incorporating the following: Creating criteria and evaluating potential trades Establishing position sizes, entry, and exit rules Minimum of 5 different assets (NOT asset classes) Use of leverage Produce a \u0026ldquo;paper\u0026rdquo; in Jupyter Notebook, HTML Produce a PDF pitch book From the abstract: The final project seeks to develop a systematic trading/investment strategy that takes advantage of the time-tested market ideas that the market tends to increase in price/value over time, positive earnings growth tends to increase company valuation, companies with positive earnings growth tend to exhibit upward price momentum. When companies have lower earnings, they tend to have lower valuations which provides an opportunity for the value factor to take effect.\nFINM 35900 - Macro-Finance Example coursework: The final group project consisted of selecting a fund or company and improving the fund with respect to macro/multi-asset ideas. The deliverables included: A concise PDF write-up with appendicies A slide deck Final calculations in Jupyter Notebook, HTML From the abstract: This proposal examines the potential benefits of incorporating Bitcoin into the asset allocation strategy of the Illinois State Board of Investment (ISBI). With over $25 billion in assets under management and a significant responsibility for managing pension plans for State of Illinois employees, ISBI\u0026rsquo;s investment policy warrants periodic review and adjustment to optimize returns and manage risk. This study suggests initializing a 1% allocation of the fund\u0026rsquo;s assets to Bitcoin as part of a diversified portfolio, alongside current assets.\nFINM 37301 - Foreign Exchange: Markets, Products, and Pricing FINM 35910 - Applied Algorithmic Trading Example coursework: The final group project was to \u0026ldquo;develop and present a fictional fund focusing on alternative algorithmic trading strategies.\u0026rdquo; The strategy was to provide \u0026ldquo;uncorrelated return relative to the S\u0026amp;P 500 Index through robust strategy creation, risk management, and performance metrics.\u0026rdquo; Final presentation slide deck Final calculations in Jupyter Notebook, HTML From the fund prospectus: Part-Time Trading LLC is an investment firm focusing on developing machine-learning models to actively predict prices within cryptocurrency markets. Our funds objective is to extract predictive features from crypto market data and utilize these features to develop best in class predictive machine learning models. Currently, we are focusing our trading efforts on trading Bitcoin and Ethereum on the crypto exchange Kraken. As we continue to advance our proprietary research and models, we plan to expand into other cryptocurrencies and exchanges.Our trading system is designed to deliver consistent alpha by exploiting inefficiencies and opportunities in the highly volatile and liquid crypto markets. Our objective is to maximize returns while minimizing risk through data-driven decision making and robust predictive analytics.\nFINM 31200 - Blockchain and Cryptoassets for Finance FINM 32900 - Full-Stack Quantitative Finance BUSN 41204 - Machine Learning (search \u0026ldquo;41204\u0026rdquo; in the search under \u0026ldquo;Course Number\u0026rdquo;) FINM 36000 - Project Lab Jared currently plans to graduate in the spring of 2026 after completing the following courses:\nFINM 37400 - Fixed Income FINM 35700 - Credit Markets Professional Certifications #Jared currently holds the following professional certifications:\nProfessional Engineer (P.E.), Illinois Department of Financial and Professional Regulation, License Number 062073506 Project Management Professional (PMP), Project Management Institute, License Number 2595125 ","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/about-me/","section":"Pages","summary":"","title":"About Me"},{"content":"Software #Here are some links to software, etc. that I find interesting and use.\nGitHub: GitHub is the world\u0026rsquo;s largest software development platform. Gnome Apps: Apps for GNOME. Arch Linux: Arch Linux - A simple, lightweight distribution. LibreOffice: LibreOffice includes several applications that make it the most versatile Free and Open Source office suite available, including Writer (word processing), Calc (spreadsheets), Impress (presentations), Draw (vector graphics and flowcharts), Base (databases), and Math (formula editing). Master PDF Editor: Master PDF Editor is straightforward, easy to use application for working with PDF documents equipped with powerful multi-purpose functionality. With Master PDF Editor you can easily view, create and modify PDF documents. The application enables you to merge several files into one, split a source document into multiple documents, and also to comment, sign and encrypt PDF files. digiKam: digiKam is an advanced open-source digital photo management application that runs on Linux, Windows, and macOS. The application provides a comprehensive set of tools for importing, managing, editing, and sharing photos and raw files. Thunderbird: Thunderbird is a free and open source email, newsfeed, chat, and calendaring client, thats easy to set up and customize. One of the core principles of Thunderbird is the use and promotion of open standards - this focus is a rejection of our world of closed platforms and services that cant communicate with each other. We want our users to have freedom and choice in how they communicate. pandas: pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. ","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/links/","section":"Pages","summary":"","title":"Links"},{"content":"","date":null,"permalink":"https://jaredszajkowski.github.io/jaredszajkowski.github.io_congo/pages/","section":"Pages","summary":"","title":"Pages"}]